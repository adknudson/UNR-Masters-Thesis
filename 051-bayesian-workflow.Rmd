```{r ch5_1-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(patchwork)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

logit <- function(p) qlogis(p)
inv_logit <- function(x) plogis(x)
logistic <- function(x) inv_logit(x)
fn <- function(x, a, b) logistic(b * (x - a))

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


## Iteration 2 (Electric Boogaloo)

### Model Development {#mod-dev-iter2}

In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, 1.0^2) \\
\beta_{TG} &\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\mu_\beta &\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}


In the above formulation, $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2$ if it's the pre-adaptation block, and $\left(\sqrt{1.5^2 + 1.0^2}\right)^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2$ if it's the post-adaptation block. Values that are negative reduce the slope (increase the JND), and values that are positive increase the slope (reduce the JND).

But wait! this model implies that there is more uncertainty about the post-adaptation trials compared to the baseline trials, and this is not necessarily true. Furthermore, as we'll see in the linear part of model, the intercept, $\alpha$, is no longer the average response probability of the sample, but is instead exclusively the average for the pre-adaptation trials. This may not matter in certain analyses, but one nice property of multilevel models is the separation of population level estimates and group level estimates. So we modify the model for the slope to be:


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, \sigma_{\beta T}^2) \\
\mu_\beta &= \exp(\beta + \beta_G + \beta_T)
\end{align*}


Now $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2 + \sigma_{\beta T}^2$, regardless of whether it's the pre-adaptation or the post-adaptation block.

The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.


\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05^2) \\
\alpha_G &\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &\sim \mathcal{N}(0, \sigma_{\alpha T}^2) \\
\mu_\alpha &= \alpha + \alpha_{G} + \alpha_{T}
\end{align*}


The parameters and model of the Stan program is

```
parameters {
  real a;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real bG[N_G];
  real bT[N_T];
  
  real<lower=machine_precision()> sd_aG;
  real<lower=machine_precision()> sd_aT;
  real<lower=machine_precision()> sd_bG;
  real<lower=machine_precision()> sd_bT;
}
model {
  a ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  
  b ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);
  
  vector[N] p;
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[T[i]]);
    real mu_a = a + aG[G[i]] + aT[T[i]];
    p[i] = mu_b * (x[i] - mu_a);
  }
  k ~ binomial_logit(n, p);
}
```


Just like in the first iteration, we begin by simulating the observational model.


###  Simulate bayesian ensemble

```{stan output.var="av_iter2_sim"}
functions {
  real half_cauchy_rng(real sigma) {
    real u = uniform_rng(0.5, 0.99);
    real y = sigma * tan(pi() * (u - 0.5));
    return y;
  }
}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
generated quantities {
  vector[N] theta;
  int y_sim[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  real alpha = normal_rng(0, 0.05);
  real<lower=0> sigma_aG = half_cauchy_rng(0.01);
  real<lower=0> sigma_aT = half_cauchy_rng(0.01);
  real alpha_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_aG);
  real alpha_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_aT);

  real beta = normal_rng(3.0, 1.5);
  real<lower=0> sigma_bG = half_cauchy_rng(0.5);
  real<lower=0> sigma_bT = half_cauchy_rng(0.5);
  real beta_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_bG);
  real beta_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_bT);

  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + beta_T[trt[i]]);
    real delta = alpha + alpha_G[G[i]] + alpha_T[trt[i]];

    theta[i] = inv_logit( gamma * (x[i] - delta) );
    y_sim[i] = binomial_rng(n[i], theta[i]);
  }

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(beta + beta_G[i] + beta_T[j]);
      real mu_a = alpha + alpha_G[i] + alpha_T[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}

```


```{r}
dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                         G = as.integer(age_group), 
                         trt = as.integer(trial),
                         x = x, n = n)) 

m5_2_1 <- sampling(av_iter2_sim, data = dat, 
                   chains = 1, cores = 1, iter = 3000, warmup = 1000,
                   algorithm = "Fixed_param", refresh = 0)
prior5_2_1 <- extract(m5_2_1)
```

### Prior Checks

```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,2], breaks = 50, probability = TRUE)
}
```


```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,2], breaks = 50, probability = TRUE)
}
```

```{r}
apply(prior5_2_1$jnd, c(2,3), quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>% round(3)
```

### Configure algorithm

### Fit simulated ensemble

```{r}
n_sims <- length(prior5_2_1$alpha)
n_obs <- length(dat$x)
idx <- sample(1:n_sims, n_obs, replace = TRUE)
a <- prior5_2_1$alpha[idx]
b <- prior5_2_1$beta[idx]
probs <- logistic(b * (dat$x - a))
sim_k <- rbinom(n_obs, dat$n, probs)
```


```{stan output.var="av_iter2_sim_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real aG[N_G];
  real aT[N_T];

  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  real bG[N_G];
  real bT[N_T];
}
model {
  vector[N] theta;

  a  ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);

  b  ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r}
sim_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = sim_k)) 

m5_2_2 <- sampling(av_iter2_sim_fit, data = sim_dat, 
                   chains = 10, cores = 10, iter = 2500, warmup = 2000,
                   refresh = 0)
# post5_2_2 <- extract(m5_2_2)
```


### Algorithmic calibration

```{r}
check_hmc_diagnostics(m5_2_2)
```

Let's see if we can do a little better by using a reparameterized model.

```{stan output.var="av_iter2_sim_fit_nc"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  vector[N_G] bG;
  vector[N_T] bT;
  
  a = a_raw * 0.05;
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);

  b  ~ normal(3.0, 1.5);
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r}
m5_2_3 <- sampling(av_iter2_sim_fit_nc, data = sim_dat, 
                   chains = 10, cores = 10, iter = 2500, warmup = 2000,
                   refresh = 0)
post5_2_3 <- extract(m5_2_3)
```

```{r}
check_hmc_diagnostics(m5_2_3)
```

```{r}
round(summary(m5_2_3, pars = c("a", "aG", "aT"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_3, pars = c("b", "bG", "bT"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_3, pars = c("pss"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_3, pars = c("jnd"))$summary, 4)[,c(1,2,3,4,8,9,10)]
```


### Inferential Calibration

### Fit Observation

```{stan output.var="av_iter2_obs_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  vector[N_G] bG;
  vector[N_T] bT;
  
  a = a_raw * 0.05;
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);

  b  ~ normal(3.0, 1.5);
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    y_post_pred[i] = binomial_rng(n[i], inv_logit(mu_b * (x[i] - mu_a)));
  }
}
```


```{r}
obs_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = k)) 

m5_2_4 <- sampling(av_iter2_obs_fit, data = obs_dat, 
                   chains = 10, cores = 10, iter = 2500, warmup = 2000,
                   refresh = 0)
post5_2_4 <- extract(m5_2_4)
```


### Diagnose posterior fit

```{r}
check_hmc_diagnostics(m5_2_4)
```

It's looking alright!

```{r}
round(summary(m5_2_4, pars = c("a", "aG", "aT"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_4, pars = c("b", "bG", "bT"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_4, pars = c("pss"))$summary, 4)[,c(1,2,3,4,8,9,10)]
round(summary(m5_2_4, pars = c("jnd"))$summary, 4)[,c(1,2,3,4,8,9,10)]
```


### Posterior retrodictive checks

```{r}
post5_2_4_k_pred <- t(apply(post5_2_4$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_2_4$y_post_pred), 1)

m5_2_4_pred <- cbind(post5_2_4_k_pred,
                 post_mean = colMeans(post5_2_4$y_post_pred),
                 post_rand = post5_2_4$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial, age_group = av_dat$age_group) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r}
m5_2_4_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_grid(age_group ~ trial)
```

The retrodictive data are matching well with the observed data, which means that we are getting closer to a model that we can use for inferences.

```{r}
plot_pf <- function(n, post, age_group, trt) {
  n_smp <- 100
  idx <- sample(1:length(post$a), n_smp, replace = TRUE)
  
  alpha <- with(post, a[idx] + aG[idx, age_group] + aT[idx, trt])
  beta <- with(post, exp(b[idx] + bG[idx, age_group] + bT[idx, trt]))
  
  p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
    ggplot(aes(x, y)) +
    scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
    scale_y_continuous(breaks = c(0, 0.5, 1))
  for (i in 1:n_smp) {
    p <- p + geom_line(stat = "function", fun = fn, 
                       args = list(a = alpha[i],
                                   b = beta[i]),
                       alpha = 0.05)
  }
  p
}
```



```{r}
ypre <- plot_pf(100, post5_2_4, 1, 1)
ypos <- plot_pf(100, post5_2_4, 1, 2)
mpre <- plot_pf(100, post5_2_4, 2, 1)
mpos <- plot_pf(100, post5_2_4, 2, 2)
opre <- plot_pf(100, post5_2_4, 3, 1)
opos <- plot_pf(100, post5_2_4, 3, 2)

(ypre + ypos) / (mpre + mpos) / (opre + opos)
```

It's difficult to determine from this graph if there any difference between the age groups. Looking at the density plot of the PSS and JND across the different conditions paints a much clearer image.

```{r}
age_trt <- expand.grid(a = 1:3, t = 1:2)

dat_pssjnd <- lapply(1:nrow(age_trt), function(i) {
  a <- age_trt$a[i]
  t <- age_trt$t[i]
  tibble(PSS = post5_2_4$pss[,a,t],
         JND = post5_2_4$jnd[,a,t],
         a = a,
         t = t)
}) %>% do.call(what = bind_rows) %>%
  mutate(a = factor(a, levels = 1:3, labels = levels(av_dat$age_group)),
         t = factor(t, levels = 1:2, labels = levels(av_dat$trial))) %>%
  rename(age_group = a, trial = t) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds")
```


```{r}
dat_pssjnd %>%
  ggplot(aes(Seconds, fill = trial)) +
  geom_density() +
  scale_fill_manual("trial",
                    values = c(rgb(29/255, 149/255, 219/255, 0.5),
                               rgb(143/255, 19/255, 19/255, 0.5))) +
  facet_grid(age_group ~ Name, scales = "free_x")
```

Using ocular analysis^[Often referred to in the non-sciences as eyeballs] we can see that recalibration has a significant affect on the just noticeable difference for each age group; specifically recalibration heightens temporal sensitivity and thus reduces the just noticeable difference. Comparing between age groups, the young and middle age groups are very similar in both the pre- and post-adaptation trials, and temporal sensitivity is lower in the older age group.

The point of subjective simultaneity between trials is not as well separated, but the model still consistently estimates that the subjects will perceive simultaneity at a value closer to zero post-adaptation.
