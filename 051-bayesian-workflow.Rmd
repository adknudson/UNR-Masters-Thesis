```{r ch051-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(patchwork)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


## Iteration 2 (Electric Boogaloo) {#iter2}

### Model Development {#iter2-model-dev}

In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, 1.0^2) \\
\beta_{TG} &\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\mu_\beta &\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}


In the above formulation, $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2$ if it's the pre-adaptation block, and $\left(\sqrt{1.5^2 + 1.0^2}\right)^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2$ if it's the post-adaptation block. Values that are negative reduce the slope (increase the JND), and values that are positive increase the slope (reduce the JND).

But wait! this model implies that there is more uncertainty about the post-adaptation trials compared to the baseline trials, and this is not necessarily true. Furthermore, as we'll see in the linear part of model, the intercept, $\alpha$, is no longer the average response probability of the sample, but is instead exclusively the average for the pre-adaptation trials. This may not matter in certain analyses, but one nice property of multilevel models is the separation of population level estimates and group level estimates. So we modify the model for the slope to be:


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, \sigma_{\beta T}^2) \\
\mu_\beta &= \exp(\beta + \beta_G + \beta_T)
\end{align*}


Now $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2 + \sigma_{\beta T}^2$, regardless of whether it's the pre-adaptation or the post-adaptation block.

The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.


\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05^2) \\
\alpha_G &\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &\sim \mathcal{N}(0, \sigma_{\alpha T}^2) \\
\mu_\alpha &= \alpha + \alpha_{G} + \alpha_{T}
\end{align*}


The parameters and model of the Stan program is

```
parameters {
  real a;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real bG[N_G];
  real bT[N_T];
  
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
}
model {
  a ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  
  b ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);
  
  vector[N] p;
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[T[i]]);
    real mu_a = a + aG[G[i]] + aT[T[i]];
    p[i] = mu_b * (x[i] - mu_a);
  }
  k ~ binomial_logit(n, p);
}
```


Just like in the first iteration, we begin by simulating the observational model.


###  Simulate bayesian ensemble {#iter2-sim}

```{stan ch051-Iron Forgotten, output.var="av_iter2_sim"}
functions {
  real half_cauchy_rng(real sigma) {
    real u = uniform_rng(0.5, 0.99);
    real y = sigma * tan(pi() * (u - 0.5));
    return y;
  }
}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
generated quantities {
  vector[N] theta;
  int y_sim[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  real alpha = normal_rng(0, 0.05);
  real<lower=0> sigma_aG = half_cauchy_rng(0.01);
  real<lower=0> sigma_aT = half_cauchy_rng(0.01);
  real alpha_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_aG);
  real alpha_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_aT);

  real beta = normal_rng(3.0, 1.5);
  real<lower=0> sigma_bG = half_cauchy_rng(0.5);
  real<lower=0> sigma_bT = half_cauchy_rng(0.5);
  real beta_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_bG);
  real beta_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_bT);

  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + beta_T[trt[i]]);
    real delta = alpha + alpha_G[G[i]] + alpha_T[trt[i]];

    theta[i] = inv_logit( gamma * (x[i] - delta) );
    y_sim[i] = binomial_rng(n[i], theta[i]);
  }

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(beta + beta_G[i] + beta_T[j]);
      real mu_a = alpha + alpha_G[i] + alpha_T[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch051-Freaky Screwdriver}
dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                         G = as.integer(age_group), 
                         trt = as.integer(trial),
                         x = x, n = n)) 

m5_2_1 <- sampling(av_iter2_sim, data = dat, 
                   chains = 1, cores = 1, iter = 3000, warmup = 1000,
                   algorithm = "Fixed_param", refresh = 0)
prior5_2_1 <- extract(m5_2_1)
```

### Prior Checks {#iter2-prior-check}

```{r ch051-Surreal Comic}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,2], breaks = 50, probability = TRUE)
}
```


```{r ch051-Severe Kangaroo}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,2], breaks = 50, probability = TRUE)
}
```

```{r ch051-Ivory Brutal Ray}
apply(prior5_2_1$jnd, c(2,3), quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>% round(3)
```

### Configure algorithm {#iter2-config-algo}

### Fit simulated ensemble {#iter2-fit-sim}

```{r ch051-Cosmic Lone}
n_sims <- length(prior5_2_1$alpha)
n_obs <- length(dat$x)
idx <- sample(1:n_sims, n_obs, replace = TRUE)
a <- prior5_2_1$alpha[idx]
b <- prior5_2_1$beta[idx]
probs <- logistic(b * (dat$x - a))
sim_k <- rbinom(n_obs, dat$n, probs)
```


```{stan ch051-Homeless Albatross, output.var="av_iter2_sim_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real aG[N_G];
  real aT[N_T];

  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  real bG[N_G];
  real bT[N_T];
}
model {
  vector[N] theta;

  a  ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);

  b  ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch051-Western Weeknight}
sim_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = sim_k)) 

m5_2_2 <- sampling(av_iter2_sim_fit, data = sim_dat, 
                   chains = 10, cores = 10, 
                   iter = 2500, warmup = 2000,
                   refresh = 0, seed = 1)
```


### Algorithmic calibration {#iter2-algo-calibration}

```{r ch051-Intense Canal, message=TRUE}
check_hmc_diagnostics(m5_2_2)
```

Let's see if we can do a little better by reparameterizing the model, and tuning the algorithm a bit. As I'll discuss more in the [model checking](#model-checking) section, we can use the non-centered parameterization of the normal and Cauchy distributions to make it easier for the Hamiltonian Monte Carlo algorithm to explore the posterior. Additionally Stan suggests increasing the `adapt_delta` parameter to remove divergences, so we will do that. Finally, to take care of the message about R-hat and effective sample sizes, I will run the chains for more iterations.

```{stan ch051-Stormy Mountain, output.var="av_iter2_sim_fit_nc"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  real sd_aG;
  real sd_aT;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  real sd_bG;
  real sd_bT;
  
  a = a_raw * 0.05;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  
  b = 3.0 + b_raw * 1.5;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch051-Pink Morning}
keep_pars <- c(
  "a", "b",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT"
)

m5_2_3 <- sampling(av_iter2_sim_fit_nc, data = sim_dat, 
                   chains = 4, cores = 4,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   seed = 2)
```

```{r ch051-Dusty Ray, message=TRUE}
check_hmc_diagnostics(m5_2_3)
```

Excellent! Now only about half a percent of the transitions are divergent, and there are no longer any warnings about the R-hat statistic.

### Inferential Calibration {#iter2-inferential-calibration}

### Fit Observation {#iter2-fit-obs}

```{stan ch051-Temple Husky, output.var="av_iter2_obs_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  real sd_aG;
  real sd_aT;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  real sd_bG;
  real sd_bT;
  
  a = a_raw * 0.05;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  
  b = 3.0 + b_raw * 1.5;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    y_post_pred[i] = binomial_rng(n[i], inv_logit(mu_b * (x[i] - mu_a)));
  }
}
```


```{r ch051-Empty Steel}
obs_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = k)) 

keep_pars <- c(
  "a", "b",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT",
  "y_post_pred"
)

m5_2_4 <- sampling(av_iter2_obs_fit, data = obs_dat, 
                   chains = 4, cores = 4,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   seed = 2)
```


### Diagnose posterior fit {#iter2-diagnose-post}

```{r ch051-Hungry Wrench, message=TRUE}
check_hmc_diagnostics(m5_2_4)
```

It's looking alright!

```{r ch051-Green Burst}
stan_summary(m5_2_4, pars = c("a", "aG", "aT"))
stan_summary(m5_2_4, pars = c("b", "bG", "bT"))
stan_summary(m5_2_4, pars = c("pss"))
stan_summary(m5_2_4, pars = c("jnd"))
```


### Posterior retrodictive checks {#iter2-post-retro}

```{r ch051-Purple Clown}
post5_2_4 <- extract(m5_2_4)
post5_2_4_k_pred <- t(apply(post5_2_4$y_post_pred, 2, quantile,
                            probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_2_4$y_post_pred), 1)

m5_2_4_pred <- cbind(post5_2_4_k_pred,
                 post_mean = colMeans(post5_2_4$y_post_pred),
                 post_rand = post5_2_4$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial, age_group = av_dat$age_group) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r ch051-Sleepy Roadrunner}
m5_2_4_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_grid(age_group ~ trial)
```


The retrodictive data are matching well with the observed data, which means that we are getting closer to a model that we can use for inferences.


```{r ch051-Steady Canal}
plot_pf <- function(n, post, age_group, trt) {
  n_smp <- 100
  idx <- sample(1:length(post$a), n_smp, replace = TRUE)
  
  alpha <- with(post, a[idx] + aG[idx, age_group] + aT[idx, trt])
  beta <- with(post, exp(b[idx] + bG[idx, age_group] + bT[idx, trt]))
  
  p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
    ggplot(aes(x, y)) +
    scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
    scale_y_continuous(breaks = c(0, 0.5, 1))
  for (i in 1:n_smp) {
    p <- p + geom_line(stat = "function", fun = fn, 
                       args = list(a = alpha[i],
                                   b = beta[i]),
                       alpha = 0.05)
  }
  p
}
```


```{r ch051-Furious Jazz}
ypre <- plot_pf(100, post5_2_4, 1, 1)
ypos <- plot_pf(100, post5_2_4, 1, 2)
mpre <- plot_pf(100, post5_2_4, 2, 1)
mpos <- plot_pf(100, post5_2_4, 2, 2)
opre <- plot_pf(100, post5_2_4, 3, 1)
opos <- plot_pf(100, post5_2_4, 3, 2)

(ypre + ypos) / (mpre + mpos) / (opre + opos)
```


It's difficult to determine from this graph if there any difference between the age groups. Looking at the density plot of the PSS and JND across the different conditions paints a much clearer image.


```{r ch051-Discarded Torpedo}
age_trt <- expand.grid(a = 1:3, t = 1:2)

dat_pssjnd <- lapply(1:nrow(age_trt), function(i) {
  a <- age_trt$a[i]
  t <- age_trt$t[i]
  tibble(PSS = post5_2_4$pss[,a,t],
         JND = post5_2_4$jnd[,a,t],
         a = a,
         t = t)
}) %>% do.call(what = bind_rows) %>%
  mutate(a = factor(a, levels = 1:3, labels = levels(av_dat$age_group)),
         t = factor(t, levels = 1:2, labels = levels(av_dat$trial))) %>%
  rename(age_group = a, trial = t) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds")

dat_pssjnd %>%
  ggplot(aes(Seconds, fill = trial)) +
  geom_density() +
  scale_fill_manual("trial",
                    values = c(rgb(29/255, 149/255, 219/255, 0.5),
                               rgb(143/255, 19/255, 19/255, 0.5))) +
  facet_grid(age_group ~ Name, scales = "free_x")
```


Using ocular analysis^[Often referred to in the non-sciences as eyeballs] we can see that recalibration has a significant affect on the just noticeable difference for each age group; specifically recalibration heightens temporal sensitivity and thus reduces the just noticeable difference. Comparing between age groups, the young and middle age groups are very similar in both the pre- and post-adaptation trials, and temporal sensitivity is lower in the older age group.

The point of subjective simultaneity between trials is not as well separated, but the model still consistently estimates that the subjects will perceive simultaneity at a value closer to zero post-adaptation.
