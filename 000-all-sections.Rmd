# Introduction

With the advances in computational power and high-level programming languages like Python, R, and Julia, statistical methods have evolved to be more flexible and expressive. No longer must we be subjugated by p-values and step-wise regression techniques. Gone are the days of using clever modeling techniques to tame misbehaved data. Now is the time for principled and informed decisions to create bespoke models and domain-motivated analyses. We have the shoulders of giants to stand upon and look out at the vast sea of data science.

I want to talk about how the advances in computational power have lead to a sort of mini revolution - resurrection - in statistics where Bayesian modeling has gained an incredible following thanks to projects like Stan. The steady adoption of computer aided statistical workflows also brings the need for multidisciplinary techniques from numerical analysis, probability theory, statistics, computer science, visualizations, and more. And with the age of computers, there is a strong push towards reproducibility. Concepts of modular design, workflows, project history and versioning, virtual environments, and human readable code all contribute to reproducible analyses. And somehow I also want to tie in how data is immutable - raw data should (must) be treated as a constant and unchangeable entity, and merely touching it will cause data mitosis.

## Classical Approaches to Modeling

- Overview of classical modeling methods
  - generally inflexible (parametric) to be more interpretable and computationally easier
  - sometimes our assumptions about the data are invalid
    - normality, independence, heteroskedacity, etc.
  - often limited when it comes to statistical summaries and confidence intervals

Computational Power 
  --> Importance of reproducibility and workflows
  --> Classical modeling methods 
    --> Problems with classical methods
  --> Proposed solutions

- Solutions or alternatives when classical models fail
  - Bayesian inference is a powerful, descriptive, and flexible modeling framework
  - Bayes theorem is a simple model of incorporating prior information and data to produce a posterior probability or distribution
  - $P(\theta | X) \propto P(\theta) \cdot P(X | \theta)$ or $posterior \propto prior \times likelihood$
    - The prior is some distribution over the parameter space
    - The likelihood is the probability of an outcome in the sample space given a value in the parameter space
    - The posterior is the likelihood of values in the parameter space after observing values from the sample space
  - Bayesian statistics, when described without math, actually feels natural to most people
    - you hear hoof beats, you think horses, not zebras [unless you're in Africa, but that's prior information ;)]
  - The catch is that the model is not complete as written above
  - There is actually a denominator in Bayes' Theorem
    - $P(\theta | X) = \frac{P(X | \theta)\cdot P(\theta)}{\sum_i P(X | \theta_i)} = \frac{P(X | \theta)\cdot P(\theta)}{\int_\Omega P(X | \theta)d\theta}$
    - In general, the denominator is not known, or is not not easy (or possible) to calculate, but it always evaluates to a constant (hence the "proportional to")
    - The denominator acts as a scaling value that forces $P(\theta|X)$ to be a probability distribution (i.e. area under PDF is equal to 1)
    - There are simulation-based techniques that let one approximate the posterior distribution without needing to know the analytic solution to the denominator

## Proposal of New Methods

## Organization

I have organized this thesis as follows. In [Chapter 2](#motivating-data) I introduce the data set that drives the narrative and that motivates the adoption of Bayesian multilevel modeling. In [Chapter 3](#background) there is a review of common approaches approaches to modeling with psychometric data, and the benefits and drawbacks of such techniques. [Chapter 4](#bayesian-modeling) introduces Bayesian hierarchical modeling and programming frameworks for Bayesian inference. In [Chapter 5](#workflow) I describe and work through a principled Bayesian workflow for multilevel modeling. [Chapter 6](#model-checking) goes into more depth on checking the model goodness of fit and model diagnostics in a Bayesian setting. Finally in [Chapter 7](#predictive-inference) I demonstrate how to use the Bayesian model from the principled workflow for predictive inference, and use posterior predictive distributions to plot and compare models.

# Background and Motivating Data {#motivating-data}

```{r ch020-setup, include=FALSE, cache=FALSE}
library(FangPsychometric)
library(tidyverse)
library(patchwork)
library(kableExtra)
```

It was Charles Darwin who in his book _On the Origin of Species_ developed the idea that living organisms adapt to better survive in their environment. Sir Francis Galton, inspired by Darwin's ideas, became interested in the differences in human beings and in how to measure those differences. Though the dark side of statistics and hubris lead Galton to become a pioneer of eugenics, his works on studying and measuring human differences lead to the creation of psychometrics -- the science of measuring mental faculties. Around the same time that was developing his theories, Johann Friedrich Herbart was also interested in studying consciousness through the scientific method, and is responsible for creating mathematical models of the mind.

E.H. Weber built upon Herbart's work, and sought out to prove the idea of a psychological threshold. A psychological threshold is a minimum stimulus intensity necessary to activate a sensory system -- a _liminal_ stimulus. He paved the way for experimental psychology and is the namesake of _Weber's Law_ -- the change in a stimulus that will be just noticeable is a constant ratio of the original stimulus [@britannica2014editors].

$$
\frac{\Delta I}{I} = k
$$

To put this law into practice, consider holding a 1 kg weight ($I = 1$), and further suppose that we can _just_ detect the difference between a 1 kg weight and a 1.2 kg weight ($\Delta I = 0.2$). Then the constant just noticeable ratio is

$$
k = \frac{0.2}{1} = 0.2
$$

So now if we pick up a 10 kg weight, we should be able to determine how much more mass is required to just detect a difference:

$$
\frac{\Delta I}{10} = 0.2 \Rightarrow \Delta I = 2
$$

The difference between a 10 kg and a 12 kg weight should be just barely perceptible. Notice that the difference in the first set of weights is 0.2 and in the second set it is 2. Our perception of the difference in stimulus intensities is not absolute, but relative. G.T. Fechner devised the law (Weber-Fechner Law) that the strength of a sensation grows as the logarithm of the stimulus intensity.

$$S = K \ln I$$

An example to this law is to consider two light sources, one that is 100 lumens ($S_1 = K \ln 100$) and another that is 200 lumens ($S_2 = K \ln 200$). The intensity of the second light is not perceived as twice as bright, but only about 1.15 times as bright according to the Weber-Fechner law: $\theta = S_2 / S_1 \approx 1.15$. Notice that the value $K$ cancels out when calculating the relative intensity, but knowing $K$ can lead to important psychological insights; insights about differences between persons or groups of people! What biological and contextual factors affect how people perceive different stimuli? How do we measure their perception in a meaningful way? As one might expect, we can collect data from psychometric experiments, fit a model to the data from a family of functions called _psychometric functions_, and inspect key operating characteristics of those functions.

## Psychometric Experiments {#psycho-experiments}

Psychometric experiments are devised in a way to examine psychophysical processes, or the response between the world around us and our inward perceptions. A **psychometric function** relates an observerâ€™s performance to an independent variable, usually some physical quantity of a stimulus in a psychophysical task [@wichmann2001a]. Psychometric functions were studied as early as the late 1800's, and Edwin Boring published a chart of the psychometric function in The American Journal of Psychology in 1917 [@boring1917chart].


```{r ch020-chart-of-pf, fig.cap="A chart of the psychometric function. The experiment in this paper places two points on a subject's skin separated by some distance, and has them answer their impression of whether there is one point or 'two', recorded as either 'two' or 'not two'. As the separation of aesthesiometer points increases, so too does the subject's confidence in their response of 'two'. So at what separation is the impression of two points liminal?"}
knitr::include_graphics("figures/chart_of_pf.png")
```


Figure \@ref(fig:ch020-chart-of-pf) displays the key aspects of the psychometric function. The most crucial part is the sigmoid function, the S-like non-decreasing curve, which in this case is represented by the Normal CDF, $\Phi(\gamma)$. The horizontal axis represents the stimulus stimulus intensity, the separation of two points in centimeters. The vertical axis represents the probability that a subject has the impression of two points. With only experimental data, the response proportion becomes an approximation for the probability.

This leads me to talk about the type of psychometric experiment that this paper deals with called a **temporal order judgment** (TOJ) experiment. The concept is that if there are two distinct stimuli occurring nearly simultaneously then our brains will bind them into a single percept -- perceive them as happening simultaneously. Compensation for small temporal differences is beneficial for coherent multisensory experiences, particularly in visual-speech synthesis as it is necessary to maintain an accurate representation of the sources of multisensory events. The temporal asynchrony between stimuli is called the **stimulus onset asynchrony** (SOA), and the range of SOAs for which sensory signals are integrated into a global percept is called the **temporal binding window**. When the SOA grows too large then the brain segregates the two signals and the temporal order can be determined.

Our experiences in life as we age shape the mechanisms of processing multisensory signals, and some multisensory signals are integrated much more readily than others. Perceptual synchrony has been previously studied through the **point of subjective simultaneity** (PSS) -- the temporal delay between two signals at which an observer is unsure about their temporal order [@stone2001now]. The temporal binding window is the time span over which sensory signals arising from different modalities appear integrated into a global percept. A deficit in temporal sensitivity may lead to a widening of the temporal binding window and reduce the ability to segregate unrelated sensory signals. In temporal order judgment tasks, the ability to discriminate the timing of multiple sensory signals is referred to as temporal sensitivity, and is studied through the measurement of the **just noticeable difference** (JND) -- the smallest lapse in time so that a temporal order can just be determined. Figure \@ref(fig:ch020-plot-ref-pf) highlights the features through which we study psychometric functions. The PSS is defined as the point where an observer can do no better at determining temporal order than random guessing (i.e. the response probability is 50%). The JND is defined as the extra temporal delay between stimuli so that the temporal order is just able to be determined. Historically this has been defined as the difference between the 84% level and the PSS, though the upper level depends on domain expertise.


```{r ch020-plot-ref-pf, fig.cap="The PSS is defined as the point where an observer can do no better at determining temporal order than random guessing. The just noticeable difference is defined as the extra temporal delay between stimuli so that the temporal order is just able to be determined. Historically this has been defined as the difference between the 0.84 level and the PSS, though the upper level depends on domain expertise."}
ggplot(tibble(x = c(-4, 4), y = c(0, 1)), aes(x, y)) +
  geom_hline(yintercept = 0, lwd = 0.5) +
  geom_vline(xintercept = 0, lwd = 0.5) +
  stat_function(fun = plogis, args = list(location = 1), size = 1.5, 
                color = "steelblue") +
  geom_segment(aes(x = 0, xend = qlogis(0.5, 1), y = 0.5, yend = 0.5),
               lty = "dotted") +
  geom_segment(aes(x = qlogis(0.5, 1), xend = qlogis(0.5, 1), y = 0, yend = 0.5),
               lty = "dotted") +
  geom_segment(aes(x = 0, xend = qlogis(0.84, 1), y = 0.84, yend = 0.84),
               lty = "dotted") +
  geom_segment(aes(x = qlogis(0.84, 1), xend = qlogis(0.84, 1), y = 0, yend = 0.84),
               lty = "dotted") +
  annotate("segment", x = qlogis(0.5, 1), xend = qlogis(0.84, 1), 
           y = 0.25, yend = 0.25, size = 0.5, color = "gray32",
           arrow = arrow(ends = "both", type = "closed")) +
  annotate("label", x = mean(qlogis(c(0.5, 0.84), 1)), y = 0.25, label = "JND") +
  annotate("segment", 
           x = -1, xend = qlogis(0.5, 1),
           y = 0.84, yend = 0.5, color = "gray32",
           arrow = arrow(type = "closed")) +
  annotate("label", x = -1, y = 0.84, label = "PSS", hjust = 1, vjust = 0) +
  annotate("segment", x = -2, xend = -1.5, 
           y = 0.25, yend = plogis(-1.5, 1), color = "gray32",
           arrow = arrow(type = "closed")) +
  annotate("label", x = -2, y = 0.25, label = "F(x)", vjust = 0, hjust = 1) +
  scale_y_continuous(breaks = c(0, 0.5, 0.84, 1.0)) +
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +
  labs(x = "Stimulus Intensity", y = "Probability",
       title = "Components of the Psychometric Function, F")
```


Perceptual synchrony and temporal sensitivity can be modified through a baseline understanding. In order to perceive physical events as simultaneous, our brains must adjust for differences in temporal delays of transmission of both psychical signals and sensory processing [@fujisaki2004recalibration]. In some cases such as with audiovisual stimuli, the perception of simultaneity can be modified by repeatedly presenting the audiovisual stimuli at fixed time separations (called an adapter stimulus) to an observer [@vroomen2004recalibration]. This repetition of presenting the adapter stimulus is called **temporal recalibration**.

The data set that I introduce in the next section concerns temporal order judgment across various sensory modalities with a temporal recalibration component.

## Temporal Order Judgment Data {#toj-task}

The data set that I am using in this paper comes from experiments done by A.N. Scurry and Dr. Fang Jiang in the Department of Psychology at the University of Nevada. Reduced temporal sensitivity in the aging population manifests in an impaired ability to perceive synchronous events as simultaneous, and similarly more difficulty in segregating asynchronous sensory signals that belong to different sources. The consequences of a widening of the temporal binding window is considered in @scurry2019aging, as well as a complete detailing of the experimental setup and recording process. A shortened summary of the methods is provided below.

There are four different tasks in the experiment: audio-visual, visual-visual, visuo-motor, and duration, and I will refer to each task respectively as audiovisual, visual, sensorimotor, and duration. The participants consist of 15 young adults (age 20-27), 15 middle age adults (age 39-50), and 15 older adults (age 65-75), all recruited from the University of Nevada, Reno. Additionally all subjects are right handed and were reported to have normal or corrected to normal hearing and vision.

<!-- Count the number of unique subjects by age group and task
```{r}
multitask %>% 
  drop_na() %>% 
  group_by(age_group, task) %>% 
  select(sid) %>% 
  unique() %>% 
  count() %>%
  ungroup() %>%
  kable(caption = "Number of subjects in each task.", booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position", "striped")) %>%
  collapse_rows(columns = c(1), valign = "top")
```
-->


```{r ch020-multitask-data}
set.seed(4)
multitask %>%
  filter(trial %in% c("pre", "post1")) %>%
  select(-rid) %>%
  group_by(task) %>%
  slice_sample(n = 1) %>%
    ungroup() %>%
  kable(caption = "Sample of motivating data.", booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```


In the audiovisual TOJ task, participants were asked to determine the temporal order between an auditory and visual stimulus. Stimulus onset asynchrony values were selected uniformly between -500 to +500 ms with 50 ms steps, where negative SOAs indicated that the visual stimulus was leading, and positive values indicated that the auditory stimulus was leading. Each SOA value was presented 5 times in random order in the initial block. At the end of each trial the subject was asked to report if the auditory stimulus came before the visual, where a $1$ indicates that they perceived the sound first, and a $0$ indicates that they perceived the visual stimulus first.

A similar setup is repeated for the visual, sensorimotor, and duration tasks. The visual task presented two visual stimuli on the left and right side of a display with temporal asynchronies that varied between -300 ms to +300 ms with 25 ms steps. Negative SOAs indicated that the left stimulus was first, and positive that the right came first. A positive response indicates that the subject perceived the right stimulus first. The sensorimotor task has subjects focus on a black cross on a screen. When it disappears, they respond by pressing a button. Additionally, when the cross disappears, a visual stimulus was flashed on the screen, and subjects were asked if they perceived the visual stimulus before or after their button press. The latency of the visual stimulus was partially determined by individual subject's average response time, so SOA values are not fixed between subjects and trials. A positive response indicates that the visual stimulus was perceived after the button press.

The duration task presents two vertically stacked circles on a screen with one appearing right after the other. The top stimulus appeared for a fixed amount of time of 300 ms, and the bottom was displayed for anywhere between +100 ms to +500 ms in 50 ms steps corresponding to SOA values between -200 ms to +200 ms. The subject then responds to if they perceived the bottom circle as appearing longer than the top circle.


```{r ch020-toj-summary}
tibble(
  Task = c("Audiovisual", "Visual", "Sensorimotor", "Duration"),
  `Positive Response` = c("Perceived audio first", 
                          "Perceived right first", 
                          "Perceived visual first",
                          "Perceived bottom as longer"),
  `Positive SOA Truth` = c("Audio came before visual",
                           "Right came before left",
                           "Visual came before tactile",
                           "Bottom lasted longer than top")
) %>%
  kable(caption = "Summary of TOJ Tasks", booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```


Finally, after the first block of each task was completed, the participants went through an adaptation period where they were presented with the respective stimuli from each task repeatedly at fixed temporal delays, then they repeated the task. To ensure that the adaptation affect persisted, the subject were presented with the adapter stimulus at regular intervals throughout the second block. The blocks are designated as `pre` and `post1`, `post2`, etc. in the data set. In this paper I will only be focusing on the `pre` and `post1` blocks.

## Data Visualizations and Quirks

The dependent variable in these experiments is the perceived response which is encoded as a 0 or a 1, and the independent variable is the SOA value. If the response is plotted against the SOA values, then it is difficult to determine any relationship (see figure \@ref(fig:ch020-simple-response-soa-plot)). Transparency can be used to better visualize the relationships between SOA value and responses. The center plot in figure \@ref(fig:ch020-simple-response-soa-plot) uses the same data as the left plot, except that the transparency is set to 0.05. As a result, one can see that there is a higher density of "0" responses towards more negative SOAs, and a higher density of "1" responses for more positive SOAs. Taking it a step further, I can compute and plot the proportion of responses for a given SOA. This is displayed in the right plot. Now the relationship between SOA value and responses is clear -- as the SOA value goes from more negative to more positive, the proportion of positive responses increases from near 0 to near 1.


```{r ch020-simple-response-soa-plot, fig.cap="Left: Simple plot of response vs. soa value. Center: A plot of response vs. soa with transparency. Right: A plot of proportions vs. soa with transparency."}
p1 <- visual %>%
  filter(trial == "pre") %>%
  ggplot(aes(soa, response)) +
  geom_point(size = 2) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank())

p2 <- visual %>%
  filter(trial == "pre") %>%
  ggplot(aes(soa, response)) +
  geom_point(size = 2, alpha = 0.05) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank())

p3 <- visual_binomial %>%
  filter(trial == "pre") %>%
  mutate(p = k / n) %>%
  ggplot(aes(soa, p)) +
  geom_point(size = 2, alpha = 0.05) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank())

(p1 | p2 | p3) + plot_annotation(title = "Three plots, same data",
                                 subtitle = "Visual Task - Pre adaptation")
```


Subjectively the right plot in figure \@ref(fig:ch020-simple-response-soa-plot) is the easiest to interpret. Because of this, I will often present the observed and predicted data using the proportion of responses rather than the actual response. Proportional data also has the advantage of being bounded on the same interval as the response. For the audiovisual task, the responses can be aggregated into binomial data -- the number of positive responses for given SOA value -- which is sometimes more efficient to work with than the Bernoulli data (see table \@ref(tab:ch020-av-bin-sample)). However the number of times an SOA is presented varies between the pre-adaptation and post-adaptation blocks; 5 and 3 times per SOA respectively. 


```{r ch020-av-bin-sample}
set.seed(5)
audiovisual_binomial %>%
  select(-rid) %>%
  filter(trial %in% c("pre", "post1")) %>%
  group_by(trial) %>%
  slice_sample(n = 3) %>%
  ungroup() %>%
  select(trial, soa, n, k) %>%
  mutate(proportion = k / n) %>%
  kable(digits = 2, caption = "Audiovisual task with aggregated responses.", booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position")) %>%
  collapse_rows(columns = 1, valign = "top")
```


Other quirks about the data pertain to the subjects. There is one younger subject that did not complete the audiovisual task, and one younger subject that did not complete the duration task. Additionally there is one older subject who's response data for the post-adaptation audiovisual task is unreasonable^[By unreasonable, I mean that it is extremely unlikely that the data represents genuine responses.] (see figure \@ref(fig:ch020-av-post1-O-f-CE-plot)).

```{r ch020-av-post1-O-f-CE-plot, fig.cap="Post-adaptation response data for O-f-CE"}
audiovisual_binomial %>%
  filter(rid == "av-post1-O-f-CE") %>%
  mutate(p = k / n) %>%
  ggplot(aes(soa, p)) +
  geom_point(size = 2, alpha = 0.9) + 
  labs(title = "Proportion of responses vs. SOA",
       subtitle = "Subject ID O-f-CE",
       y = "response proportion") +
  scale_y_continuous(limits = c(0, 1))
```

It is unreasonable because, of all the negative SOAs, there were only two correct responses^[Correct in the sense that the perceived order matches the actual order.]. If a subject is randomly guessing the temporal order, then a naive estimate for the proportion of correct responses is 0.5. If a subject's proportion of correct responses is above 0.5, then they are doing better than random guessing. In figure \@ref(fig:ch020-av-post-neg-trials) it is seen that subject O-f-CE is the only one who's proportion is below 0.5 (and by a considerable amount).

```{r ch020-av-post-neg-trials, fig.asp=1, fig.cap="Proportion of correct responses for negative SOA values during the post-adaptation audiovisual experiment."}
audiovisual %>%
  filter(trial == "post1", soa < 0) %>%
  mutate(is_neg = soa < 0,
         neg_resp = !as.logical(response),
         correct = is_neg & neg_resp,
         sid = fct_reorder(sid, as.integer(age_group))) %>%
  arrange(sid) %>%
  group_by(sid) %>%
  add_count() %>%
  summarise(k = sum(correct), p = k / n) %>%
  mutate(less0.5 = p < 0.5) %>%
  ggplot(aes(p, sid, shape = `less0.5`)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  labs(title = "Proportion of correct responses for negative SOA values",
       subtitle = "Audiovisual experiment, post-adaptation block",
       x = "Proportion of correct responses",
       y = "Subject ID") +
  theme(legend.position = "none") +
  annotate("text", x = 0.49, y = 40, label = "Random guessing", 
           angle = 90, hjust = 1, vjust = 0) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25))
```

The consequences of leaving in this experimental block in the data is considered in the [Chapter 5](#workflow), but it is a clear outlier that must be noted. When this method of detecting outliers is repeated for all tasks and blocks, then I end up with 17 records in total (see figure \@ref(fig:ch020-naive-prop-outliers)), one of which is the aforementioned subject.

```{r ch020-naive-prop-outliers, fig.cap="Proportion of correct responses across all tasks and blocks Proportions are calculated individually for positive and negative SOAs."}
multitask %>%
  mutate(is_pos = soa > 0,
         is_neg = soa < 0,
         resp_pos = response == 1,
         resp_neg = response == 0) %>%
  filter(is_pos | is_neg, trial %in% c("pre", "post1")) %>%
  mutate(correct = (is_pos & resp_pos) | (is_neg & resp_neg)) %>%
  group_by(rid, is_pos) %>%
  add_count() %>%
  summarise(k = sum(correct), p = k / n) %>%
  filter(p < 0.5) %>%
  distinct() %>%
  ggplot(aes(p, rid)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  labs(title = "Proportion of correct responses",
       subtitle = "All tasks and blocks",
       x = "Proportion of correct responses",
       y = "Record ID") +
  theme(legend.position = "none") +
  annotate("text", x = 0.51, y = 8, label = "Random guessing", 
           angle = 270, vjust = 0) +
  scale_x_continuous(limits = c(0, 0.65), breaks = seq(0, 1, 0.25))
```

Most of the records that are flagged by this method of outlier detection are from the sensorimotor task, and none are from the visual task. This may be caused by the perceived difficulty of the task. One consequence of higher temporal sensitivity is that it is easier to determine temporal order. It may also be that determining temporal order is inherently easier for certain multisensory tasks than others. Since the sensorimotor task does not have fixed SOA values like the other tasks, it may be perceived as more difficult. Or perhaps the mechanisms that process touch and visual signals are not as well coupled as those that process audio and visual signals. I'll consider how to handle the sensorimotor outliers in the model fitting process.

```{r ch030-setup}
library(ggplot2)
library(magrittr)
```


# Modeling the Psychometric Function {#background}

A psychometric function can be fit using a generalized linear model (GLM) with the link function coming from the family of S-shaped curves called a sigmoid. Commonly GLMs are fit using maximum likelihood estimation (MLE). In the case of a psychometric experiment, we can represent the outcome of a single trial as the result of a random experiment arising from a Bernoulli process. Without loss of generality, the psychometric function, $F(x; \theta)$, determines the probability that the outcome is 1.

$$
\begin{align*}
Y &\sim \textrm{Bernoulli}(\pi) \\
\pi &= P(Y=1 \vert x; \theta) = F(x; \theta)
\end{align*}
$$

If $P(Y=1 | x; \theta) = F(x;\theta)$, then $P(Y = 0 | x; \theta) = 1 - F(x;\theta)$, and hence the probability of an outcome is

$$
\begin{equation}
  P(Y=y | x; \theta) = F(x;\theta)^y(1-F(x;\theta))^{1-y}
  (\#eq:bernproby)
\end{equation}
$$

The likelihood function, $\mathcal{L}$, can be determined using equation \@ref(eq:bernproby)

$$
\begin{equation}
  \begin{split}
    \mathcal{L}(\theta | y, x) &= \prod_{i}^{N} P(y_i | x_i; \theta) \\
    &= \prod_{i}^{N}F(x_i;\theta)^{y_i}(1-F(x_i;\theta))^{1-y_i}
  \end{split}
  (\#eq:bernlik)
\end{equation}
$$

Equation \@ref(eq:bernlik) is commonly expressed in terms of its logarithm.

$$
\begin{equation}
  \ln \mathcal{L}(\theta | y, x) = \sum_{i}^{N} y_i \ln\left(F(x_i;\theta)\right) + (1-y_i) \ln\left(F(x_i;\theta))\right)
  (\#eq:bernloglik)
\end{equation}
$$

From here the classical approach would be to take the derivative of \@ref(eq:bernloglik), set it equal to $0$, and solve for $\theta$. Additionally one might perform a second derivative test to ensure that the solution is indeed a maximizer to \@ref(eq:bernloglik) (and hence to  \@ref(eq:bernlik)). However no closed form expression exists for the solution to $\frac{d}{d\theta} \ln \mathcal{L}(\theta) = 0$, and so numerical optimization methods like gradient descent are used to iteratively find the MLE solution. In most cases this works fine, but there are common situations where MLE fails. In this chapter I will discuss common techniques for fitting psychometric functions, the reasons to use these methods, and the conditions for when they are not optimal.

As introduced above, the first method is to use generalized linear models. A GLM is one that can be transformed into a linear model via a link function. An example of this is the logistic function which takes $x \in \mathbb{R}$ and squishes the output to be in $(0, 1)$.

$$
\begin{equation}
  F(\theta) = \frac{1}{1 + \exp\left(-\theta\right)}
  (\#eq:logistic)
\end{equation}
$$

Since $F$ is a strictly increasing and continuous function, it has an inverse, and the link for \@ref(eq:logistic) is the log-odds or logit function.

$$
\begin{equation}
  F^{-1}(\pi) = \mathrm{logit}(\pi) = \ln\left(\frac{\pi}{1 - \pi}\right)
  (\#eq:logit)
\end{equation}
$$

By taking $(F^{-1} \circ F)(\theta)$ we can arrive at a relationship that is linear in $\theta$.

$$
\begin{align*}
  \pi = F(\theta) \Longleftrightarrow F^{-1}(\pi) &= F^{-1}(F(\theta)) \\
  & = \ln\left(\frac{F(\theta)}{1 - F(\theta)}\right) \\
  &= \ln(F(\theta)) - \ln(1 - F(\theta)) \\
  &= \ln\left(\frac{1}{1 + \exp(-\theta)}\right) - \ln\left(\frac{\exp(-\theta)}{1 + \exp(-\theta)}\right) \\
  &= - \ln(1 + \exp(-\theta)) - \ln(\exp(-\theta)) + \ln(1 + \exp(-\theta)) \\
  &= - \ln(\exp(-\theta)) \\
  &= \theta
\end{align*}
$$

Linear models are favored in statistics because they are generally easier to interpret than other types of models. For example, say $\mathrm{logit}(\pi) = 0.5 + 1.5x$ where $\pi$ is the probability of a positive outcome, $Y=1$. We can say that at $x = 0$, the log-odds of a positive outcome is 0.5, and an increase in $x$ by 1 increases the log-odds by 1.5.

There is nothing that is particular about the logistic function that makes it the only suitable sigmoid for psychometric functions. Other common link functions include the probit (inverse of the standard normal CDF) and Weibull inverse CDF. In fact, any continuous CDF defined on the real number line can be used for psychometric functions. A PF only requires that a function is strictly increasing and bounded between 0 and 1.The logit and probit links have properties such as connections to log-odds and the normal distribution that make working with them more convenient.

## Methods for working with PFs

- Generalized Linear Models
  - classical approaches to fitting/estimation
    - Maximum likelihood estimation
      - Simple and almost every piece of statistical software will have an implementation
  - Expectation Maximization
  - Random effects (Gelmen and Hill)
  - Bayesian GLMs
    - Completely reliant on MCMC

Above we introduced GLMs as a method for estimating the properties of psychometric functions. GLMs are abundant in statistical analyses, and almost every piece of statistical software will have an implementation for working with them - most commonly through maximum likelihood estimation. MLE is not the only method; there is also the expectation-maximization (EM) algorithm and Bayesian GLMs.

<!-- How much do I need to know about EM algorithm? -->

<!-- How much do I need to know about Bayesian GLMs? -->

Bayesian methods for fitting psychometric function are also common

- Bayesian logistic regression
  - @gelman2008weakly
  - Can't completely express the structure (hierarchy) of the data

We can also throw away GLMs entirely and utilize non-parametric techniques for estimating psychometric functions. @zchaluk2009model make the argument that the true model for a PF is rarely (if ever) known, so it is better to make no assumptions about the underlying performance function - a requirement for GLMs. They show that their model-free estimation yields better-if-not-equal results in signal detection than parametric models, but the tradeoff is the loss of inferential power and the ability to generalize predictions to new groups or previously unobserved data.

## Post-model Analysis

- Residual Analysis
  - using the fitted values vs. the observed values to evaluate goodness of fit
- ANOVA
- T-tests

## Shortcomings

Despite the proliferation of maximum likelihood estimation for psychometric functions, there are also common situations where MLE is sub-optimal or not possible. The most common example given is when the data are perfectly separable. Perfect separation is when all the positive responses can be determined by a set of predictors with no contamination (see figure \@ref(fig:ch030-perfect-separation)).

```{r ch030-perfect-separation, fig.cap="The positive responses on the right can be completely separated from the negative responses on the left by any value of x between 1 and 2."}
data.frame(x = seq(-6, 6, 1),
           y = c(rep(0, 8), rep(1, 5))) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 2) +
  geom_vline(xintercept = c(1, 2), alpha = 0.5, linetype = "dashed") +
  annotate("segment", 
           x = 1, xend = 2, 
           y = 0.5, yend = 0.5,
           color = "gray32", 
           arrow = arrow(length = unit(0.2, "cm"), 
                         type = "closed", 
                         ends = "both")) +
  scale_x_continuous(breaks = seq(-6, 6, 1), minor_breaks = NULL) +
  labs(title = "Responses with perfect separation")
```

Two problems happen here: 1) the maximum likelihood estimate of the slope is $\infty$ which means that some algorithms will never converge, and 2) the center of the psychometric function is not uniquely defined. Any location value in the range $[1, 2]$ is valid and perfectly separates the positive and negative responses. In psychometric experiments, neither of these results are likely, and neither are informative. The just noticeble *difference* is not zero, and the *point* of subject simultaneity is not a range.

- shortcomings
  - Convergence failure in the presence of complete separation
    - [@prins2019too], [@ghosh2018use]
    - Give an example with how the MLE values are estimated
    - Give an example where MLE fails
    
- problems/limitations of post-model analyses
- problems with thinking that simply using Bayesian techniques solves all problems with classical ones


- So what's the answer?
  - The last two options (bayes + multilevel) when on their own do well, but are not robust to 

## Bayesian Multilevel Modeling {#bayesian-modeling}

- short intro
  - sentence 1
  - sentence 2
  - sentence 3

### Bayesian Stuff

- Mathematical foundations
  - Bayes rule in regression setting
  
- Easy in theory, difficult in practice
  - Example of a conjugate priors
  - need more complexity -> computer methods
  - Computer methods needed
-

### Multilevel Modeling Stuff

- Estimating the variance at different levels in the model
- 

```{r ch040-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


# Principled Bayesian Workflow {#workflow}

There are many great resources out there^[citation needed] for following along with an analysis of some data or problem, and much more is the abundance of tips, tricks, techniques, and testimonies to good modeling practices. The problem is that many of these prescriptions are given without context for when they are appropriate to be taken. According to @betancourt2020, this leaves "practitioners to piece together their own model building workflows from potentially incomplete or even inconsistent heuristics." The concept of a principled workflow is that for any given problem, there is not, nor should there be, a default set of steps to take to get from data exploration to predictive inferences. Rather great consideration must be given to domain expertise and the questions that one is trying to answer with the data.

Since everyone asks different questions, the value of a model is not in how well it ticks the boxes of goodness-of-fit checks, but in consistent it is with domain expertise and its ability to answer the unique set of questions. Betancourt suggests answering four questions to evaluate a model by:

1. Domain Expertise Consistency - Is our model consistent with our domain expertise?
2. Computational Faithfulness - Will our computational tools be sufficient to accurately fit our posteriors?
3. Inferential Adequacy - Will our inferences provide enough information to answer our questions?
4. Model Adequacy - Is our model rich enough to capture the relevant structure of the true data generating process?

<br />

- Scope out your problem
- Specify likelihood and priors
- check the model with fake data
- fit the model to the real data
- check diagnostics
- graph fit estimates
- check predictive posterior
- compare models

## Iteration 1 (The Journey of a Thousand Miles Begins with a Single Step) {#iter1}

### Pre-Model, Pre-Data

We begin the modeling process by modeling the experiment according to the description of how it occurred and how the data were collected. This first part consists of conceptual analysis, defining the observational space, and constructing summary statistics that can help us to identify issues in the model specification.

#### Conceptual analysis {#iter1-concept}

In section \@ref(toj-task) we discussed the experimental setup and data collection. To reiterate, subjects are presented with two stimuli separated by some temporal delay, and they are asked to respond as to their perception of the temporal order. There are 45 subjects with 15 each in the young, middle, and older age groups. As the SOA becomes larger in the positive direction, we expect subjects to give more "positive" responses, and as the SOA becomes larger in the negative direction, we expect more "negative" responses. By the way the experiment and responses are constructed, we would not expect to see a reversal of this trend unless there was an issue with the subject's understanding of the directions given to them or an error in the recording device.

We also know that after the first experimental block the subjects go through a recalibration period, and repeat the experiment again. We are interested in seeing if the recalibration has an effect on temporal sensitivity and perceptual synchrony, and if the effect is different for each age group.

#### Define observational space {#iter1-obs-space}

The response that subjects give to a TOJ task is recorded as a zero or a one (see section \@ref(toj-task)), and their relative performance is determined by the SOA value. Let $y$ represent the binary outcome of a trial and let $x$ be the SOA value.

\begin{align*}
y_i &\in \lbrace 0, 1\rbrace \\
x_i &\in \mathbb{R}
\end{align*}

If the SOA values are fixed like in the audiovisual task, then the responses can be aggregated into binomial counts, $k$.

$$
k_i, n_i \in \mathbb{Z}_0^+, k_i \le n_i
$$

In the above equation, $\mathbb{Z}_0^+$ represents the set of non-negative integers. Notice that the number of trials $n$ has an index variable $i$. This is because the number of trials per SOA is not fixed between blocks. In the pre-adaptation block, there are five trials per SOA compared to three in the post-adaptation block. So if observation 32 is recorder during a "pre" block, $n_{32} = 5$, and if observation 1156 is during a "post" block, $n_{1156} = 3$.

Then we also have the three categorical variables -- age group, subject ID, and adaptation. For the first two, we treat them as factor variables^[Factor variables also go by the name index variable or categorical variable]. Rather than using one-hot encoding or dummy variables, we leave the age levels as categories and fit a coefficient for each level. Among the benefits of this approach is the ease of interpretation and ease of working with the data programmatically. This is especially true at the subject level. If we used dummy variables for all 45 subjects, we would have 44 different dummy variables to work with, times the number of coefficients that make estimates at the subject level. In the final iteration of our model, this can be as many as $44 \times 4 = 176$ dummy variables for the subject level!

Age groups and individual subjects can be indexed in the same way that we index the number of trials. $S_i$ refers to the subject in record $i$, and similarly $G_i$ refers to the age group of that subject. Observation 63 is for record ID `r FangPsychometric::audiovisual_binomial$rid[63]`, so then $S_{63}$ is `r FangPsychometric::audiovisual_binomial$sid[63]` and $G_{63}$ is `r FangPsychometric::audiovisual_binomial$age_group[63]`. Under the hood of R, these factor levels are represented as integers (e.g. middle age group level is stored internally as the number 2).

We treat the pre- and post-adaptation categories as a binary indicator referred to as $trt$ (short for treatment) since there are only two levels in the category. In this setup, a value of 1 indicates a post-adaptation block. We chose this encoding over the reverse because the pre-adaptation block is like the baseline performance, and it seemed more appropriate to interpret the post-adaptation block as turning on some effect. Using a binary indicator in a regression setting may not be the best practice as we discuss in section \@ref(iter2-model-dev).

We will be using the Stan probabilistic programming language to estimate the model for our data. In the Stan modeling language, data for a binomial model with subject and age group levels and treatment is specified as

\setstretch{1.0}
```
data {
  int N;        // Number of observations
  int N_S;      // Number of subject levels
  int N_G;      // Number of age group levels
  int N_T;      // Number of treatment/control groups
  int n[N];     // Trials per SOA
  int k[N];     // binomial counts
  vector[N] x;  // SOA values
  int S[N];     // Subject identifier
  int G[N];     // Age group identifier
  int trt[N];   // Treatment indicator
}
```
\setstretch{2.0}

However, in most of this paper we will be using the `rethinking` package [@R-rethinking] which is a high level wrapper around `rstan`. For most of the model fitting and analyses, it is sufficient. For more complex routines and for finer control, we will utilize `rstan` directly.

#### Construct summary statistics {#iter1-sum-stats}

In order to effectively challenge the validity of our model, we construct a set of summary statistics that help answer the questions of domain expertise consistency and model adequacy. We are studying the affects of age and temporal recalibration through the PSS and JND (see section \@ref(psycho-experiments)), so it is natural to define summary statistics around these quantities to verify model consistency. Additionally the PSS and JND can be computed regardless of the model parameterization or chosen psychometric function.

By the experimental setup and recording process, it is impossible that a properly conducted block would result in a JND less than 0 (i.e. the psychometric function is always non-decreasing), so that can be a lower limit for its threshold. On the other end it is unlikely that it will be beyond the limits of the SOA values, but even more concrete, it seems unlikely (though not impossible) that the just noticeable difference would be more than a second.

<!-- As for the point of subjective simultaneity, it can be either positive or negative, with the belief that larger values are more rare. Some studies suggest that for audio-visual TOJ tasks, the separation between stimuli need to be as little as 20 milliseconds for subjects to be able to determine which modality came first [@vatakis2007influence]. Other studies suggest that our brains can detect temporal differences as small as 30 milliseconds. If we take these studies to heart, then we should be skeptical of PSS estimates larger than say 150 milliseconds in absolute value, just to be safe. -->

A histogram of computed PSS and JND values will suffice for summary statistics. We can estimate the proportion of values that fall outside of our limits defined above, and use them as indications of problems with the model fitting or our conceptual understanding.

<!-- We can further refine the lower bound on the JND if we draw information from other sources. Some studies show that we cannot perceive time differences below 30 ms, and others show that an input lag as small as 100ms can impair a person's typing ability. -->

### Post-Model, Pre-Data

We will now define priors for our model, still not having looked at the data. The priors should be motivated by domain expertise and *prior knowledge*, not the data.

#### Model development {#iter1-model-dev}

Talk here about linear parameterization and connection to PSS and JND.

Talk here about how I select priors for the intercept (PSS) and the slope (JND). Choose a standard deviation for intercept so that $\approx 95\%$ of the values are between $\pm 0.1$

\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05) \\
\beta &\sim \mathrm{Lognormal}(3.96, 1.2)
\end{align*}

If the expected JND is 0.100 (100 ms) and is distributed log-normally, then $\mathrm{logit}(0.84)/jnd$ is also log-normally distributed with mean $\mathrm{logit}(0.84) - log(0.1) \approx 3.96$.

Choose a standard deviation value so that $\approx 99\%$ of the JND values are less than 1.

The distribution of prior psychometric functions now looks like

```{r ch040-prior-pf-plot, fig.cap="Prior distribution of psychometric functions using the priors for slope and intercept."}
n <- 100
a <- rnorm(n, 0, 0.05)
b <- rlnorm(n, 3.96, 1.7)

fn <- function(x, a, b) logistic(b * (x - a))

p <- data.frame(x = c(-0.5, 0.5), y = c(0, 1)) %>% ggplot(aes(x, y))
for (i in 1:n) {
  p <- p +
    geom_function(fun = fn, args = list(a = a[i], b = b[i]),
                  color = "steelblue4", alpha = 0.15)
}

p
```


Notice that the family of psychometric functions covers the broad range of possible slopes and intercepts, though the prior distribution appears to put more weight on steeper slopes (smaller JNDs). There is also too much possibility that the PF is nearly flat. We can reduce the mean-log and sd-log of the slope parameter and get a much more uniform-looking distribution of prior psychometric curves.


\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05) \\
\beta  z&\sim \mathrm{Lognormal}(3.0, 1.5)
\end{align*}


```{r ch040-prior-pf-plot-2, fig.cap="Second prior distribution of psychometric functions using the priors for slope and intercept."}
b <- rlnorm(n, 3.0, 1.5)

p <- data.frame(x = c(-0.5, 0.5), y = c(0, 1)) %>% ggplot(aes(x, y))
for (i in 1:n) {
  p <- p +
    geom_function(fun = fn, args = list(a = a[i], b = b[i]),
                  color = "steelblue4", alpha = 0.15)
}

p
```

This prior distribution is much more reasonable. There is good prior coverage of both very steep slopes and very shallow slopes, but not so wide that nearly flat or nearly vertical slopes are likely. Also notice how the spread around $y=0.5$ remains the same independent of the slope values. This is because of how the model is parameterized. If instead we parameterized the linear predictor as 

$$
\mathrm{logit}(\pi) = \alpha^* + \beta^* \times x
$$

then the PSS would depend on both $\alpha^*$ and $\beta^*$

$$
\mathrm{PSS}^* = -\frac{\alpha^*}{\beta^*}
$$

while the JND would remain the same

$$
\mathrm{JND}^* = \mathrm{logit}(0.84)/\beta^*
$$

The problem is that it is much harder to define priors for the slope and intercept when they are so closely coupled, and the interpretation of the parameters becomes more difficult as well.

We can now extend the Stan program to include the parameters and model.

\setstretch{1.0}
```
parameters {
  real alpha;          // Intercept (PSS)
  real<lower=0> beta;  // Slope (logit(0.84) / JND)
}
model {
  alpha ~ normal(0, 0.05);      // Prior for intercept
  beta ~ lognormal(3.0, 1.5);   // Prior for slope
  vector[N] p;                  // Binomial probability
  for (i in 1:N) {
    p[i] = beta * (x[i] - alpha);
  }
  k ~ binomial_logit(n, p); // Observational model
}
```
\setstretch{2.0}

#### Construct summary functions {#iter1-summary-funs}

NA

#### Simulate bayesian ensemble {#iter1-sim}

What is the purpose of this step? To make sure that the generating model coupled with the summary stats/functions yields prior estimates that are consistent with domain expertise (see \@ref(iter1-prior-check)).

\setstretch{1.0}
```{stan ch040-Pointless Doorstop, output.var="av_iter1_sim", echo=TRUE}
data {
  int<lower=0> N;
  int n[N];
  vector[N] x;
}
generated quantities {
  real alpha = normal_rng(0, 0.05);
  real beta = lognormal_rng(3.0, 1.5);
  vector[N] theta = inv_logit( beta * (x - alpha) );
  int y_sim[N] = binomial_rng(n, theta);
  real pss = alpha;
  real jnd = logit(0.84) / beta;
}
```
\setstretch{2.0}


```{r ch040-Square Autumn}
dat <- with(av_dat, list(N = N, x = x, n = n)) 

m5_1_1 <- sampling(av_iter1_sim, data = dat, 
                   chains = 1, cores = 1, iter = 3000, warmup = 1000,
                   algorithm = "Fixed_param", refresh = 0)
prior5_1_1 <- extract(m5_1_1)
```


#### Prior checks {#iter1-prior-check}

> If the prior predictive checks indicate conflict between the model and our domain expertise then we have to return to step four [(model development)] and refine our model.


```{r ch040-Long Supersonic Cosmic}
hist(prior5_1_1$pss, breaks=50)
hist(prior5_1_1$jnd, breaks=50)
quantile(prior5_1_1$jnd, probs=c(0.95, 0.99, 0.999, 1))
```


We're satisfied with the prior coverage of the PSS and JND, so now we can move on to fitting the model to the simulated data.

#### Configure algorithm {#iter1-config-algo}

As a default, we will be using the `rethinking` package [@rethinking].

#### Fit simulated ensemble {#iter1-fit-sim}

```{r ch040-Magenta Heart}
n_sims <- length(prior5_1_1$alpha)
n_obs <- length(dat$x)
idx <- sample(1:n_sims, n_obs, replace = TRUE)
a <- prior5_1_1$alpha[idx]
b <- prior5_1_1$beta[idx]
probs <- logistic(b * (dat$x - a))
sim_k <- rbinom(n_obs, dat$n, probs)
```


\setstretch{1.0}
```{stan ch040-Maroon Vulture, output.var="av_iter1_sim_fit", echo=TRUE}
data {
  int N;
  int n[N];
  int k[N];
  vector[N] x;
}
parameters {
  real alpha;
  real<lower=0> beta;
}
model {
  vector[N] p = beta * (x - alpha);
  alpha ~ normal(0, 0.05);
  beta ~ lognormal(3.0, 1.5);
  k ~ binomial_logit(n, p);
}
generated quantities {
  real pss = alpha;
  real jnd = logit(0.84) / beta;
}

```
\setstretch{2.0}


```{r ch040-Minimum Artificial}
sim_dat <- with(av_dat, list(N = N, x = x, n = n, k = sim_k)) 

m5_1_2 <- sampling(av_iter1_sim_fit, data = sim_dat, 
                   chains = 10, cores = 10, iter = 2500, warmup = 2000,
                   refresh = 0)
post5_1_2 <- extract(m5_1_2)
```



#### Algorithmic calibration {#iter1-algo-calibration}

Did the algorithm perform correctly? What kind of diagnostics exist for this algorithm?


\setstretch{1.0}
```{r ch040-Persistent Hook, message=TRUE}
check_hmc_diagnostics(m5_1_2)
```
\setstretch{2.0}


```{r ch040-Cloudy Toupee}
stan_summary(m5_1_2, c("alpha", "beta", "pss", "jnd"))
```

- Using HMC
  - $\hat{R}$
  - Divergences
  - Effective sample size
  - Tail effective sample size
  - Bulk effective sample size
  - Bayesian fraction of missing information

Is there anything we can tune during the fitting process that can alleviate algorithmic issues? Or is it a case of Folk Theorem, and we need to adjust the model?

#### Inferential calibration {#iter1-inferential-calibration}

Non-identifiable model??

> In either case we might have to return to Step One to consider an improved experimental design or tempered scientific goals. Sometimes we may only need to return to Step Four to incorporate additional domain expertise to improve our inferences.

### Post-Model, Post-Data

#### Fit observation {#iter1-fit-obs}


\setstretch{1.0}
```{stan ch040-Golden Locomotive, output.var="av_iter1_obs_fit", echo=TRUE}
data {
  int N;
  int n[N];
  int k[N];
  vector[N] x;
}
parameters {
  real alpha;
  real<lower=0> beta;
}
model {
  vector[N] p = beta * (x - alpha);
  alpha ~ normal(0, 0.05);
  beta ~ lognormal(3.0, 1.5);
  k ~ binomial_logit(n, p);
}
generated quantities {
  real pss = alpha;
  real jnd = logit(0.84) / beta;
  int y_post_pred[N];
  for (i in 1:N)
    y_post_pred[i] = binomial_rng(n[i], inv_logit(beta * (x[i] - alpha)));
}
```
\setstretch{2.0}


```{r ch040-Homeless Tea}
obs_dat <- with(av_dat, list(N = N, x = x, n = n, k = k)) 

m5_1_3 <- sampling(av_iter1_obs_fit, data = obs_dat,
                   chains = 10, cores = 10, iter = 2500, warmup = 2000,
                   refresh = 0)
```


#### Diagnose posterior fit {#iter1-diagnose-post}

> If any diagnostics indicate poor performance then not only is our computational method suspect but also our model might not be rich enough to capture the relevant details of the observed data. At the very least we should return to Step Eight and enhance our computational method.


\setstretch{1.0}
```{r ch040-Silly Clown, message=TRUE}
check_hmc_diagnostics(m5_1_3)
```

```{r ch040-Northernmost Epsilon}
stan_summary(m5_1_3, pars = c("alpha", "beta", "pss", "jnd"))
```
\setstretch{2.0}


#### Posterior retrodictive checks {#iter1-post-retro}

Need an example of using summary stats on posterior retrodictions

```{r ch040-Mysterious Beacon Gamma}
post5_1_3 <- extract(m5_1_3)
post5_1_3_k_pred <- t(apply(post5_1_3$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_1_3$y_post_pred), 1)

m5_1_3_pred <- cbind(post5_1_3_k_pred,
                 post_mean = colMeans(post5_1_3$y_post_pred),
                 post_rand = post5_1_3$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r ch040-Angry Pineapple}
m5_1_3_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_wrap(~trial)
```

The posterior retrodictions do well to cover the observed data, but we don't actually have a model that can answer the questions that we are seeking to answer. At best, this model can only inform of of the population average PSS and JND across both pre- and post-adaptation. This first iteration does serve as a useful foundation for building a more complex model, and for practicing visualization techniques that will be more important in the next few iterations.

While we are here, let's also take a look at the underlying performance (psychometric) function as well as the density estimates of the PSS and JND.

```{r ch040-Swift Strong Xylophone}
n_smp <- 100
idx <- sample(1:length(post5_1_3$alpha), n_smp, replace = TRUE)

p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
  ggplot(aes(x, y)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  labs(title = "Underlying Psychometric Function",
       subtitle = "Average across all subjects and trials",
       x = "SOA (seconds)", y = "Probability")

for (i in idx) {
  p <- p + geom_line(stat = "function", fun = fn, 
                     args = list(a = post5_1_3$alpha[i],
                                 b = post5_1_3$beta[i]),
                     alpha = 0.05)
}
p <- p + geom_line(stat = "function", fun = fn,
               args = list(a = mean(post5_1_3$alpha),
                           b = mean(post5_1_3$beta)),
               color = "green")
p
```

We can see that the distribution of psychometric curves never wanders too far off from the mean. In this sense, we can be fairly confident that the PSS is positive.

```{r ch040-Hollow Mustard}
tibble(PSS = post5_1_3$pss, JND = post5_1_3$jnd) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds") %>%
  mutate(Name = factor(Name, levels = c("PSS", "JND"))) %>%
  ggplot(aes(Seconds)) +
  geom_density(fill = rgb(0, 1, 0, 0.25, maxColorValue = 1)) +
  facet_grid(~ Name, scales = "free_x")
```

```{r ch041-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(patchwork)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


## Iteration 2 (Electric Boogaloo) {#iter2}

### Model Development {#iter2-model-dev}

In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, 1.0^2) \\
\beta_{TG} &\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\mu_\beta &\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}


In the above formulation, $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2$ if it's the pre-adaptation block, and $\left(\sqrt{1.5^2 + 1.0^2}\right)^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2$ if it's the post-adaptation block. Values that are negative reduce the slope (increase the JND), and values that are positive increase the slope (reduce the JND).

But wait! this model implies that there is more uncertainty about the post-adaptation trials compared to the baseline trials, and this is not necessarily true. Furthermore, as we'll see in the linear part of model, the intercept, $\alpha$, is no longer the average response probability of the sample, but is instead exclusively the average for the pre-adaptation trials. This may not matter in certain analyses, but one nice property of multilevel models is the separation of population level estimates and group level estimates. So we modify the model for the slope to be:


\begin{align*}
\beta &\sim \mathcal{N}(3.0, 1.5^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, \sigma_{\beta T}^2) \\
\mu_\beta &= \exp(\beta + \beta_G + \beta_T)
\end{align*}


Now $\mu_\beta$ is a log-normal random variable with mean-log $3.0$ and variance-log $1.5^2 + \sigma_{\beta G}^2 + \sigma_{\beta T}^2$, regardless of whether it's the pre-adaptation or the post-adaptation block.

The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.


\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05^2) \\
\alpha_G &\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &\sim \mathcal{N}(0, \sigma_{\alpha T}^2) \\
\mu_\alpha &= \alpha + \alpha_{G} + \alpha_{T}
\end{align*}


The parameters and model of the Stan program is

```
parameters {
  real a;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real bG[N_G];
  real bT[N_T];
  
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
}
model {
  a ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  
  b ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);
  
  vector[N] p;
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[T[i]]);
    real mu_a = a + aG[G[i]] + aT[T[i]];
    p[i] = mu_b * (x[i] - mu_a);
  }
  k ~ binomial_logit(n, p);
}
```


Just like in the first iteration, we begin by simulating the observational model.


###  Simulate bayesian ensemble {#iter2-sim}

```{stan ch041-Iron Forgotten, output.var="av_iter2_sim"}
functions {
  real half_cauchy_rng(real sigma) {
    real u = uniform_rng(0.5, 0.99);
    real y = sigma * tan(pi() * (u - 0.5));
    return y;
  }
}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
generated quantities {
  vector[N] theta;
  int y_sim[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  real alpha = normal_rng(0, 0.05);
  real<lower=0> sigma_aG = half_cauchy_rng(0.01);
  real<lower=0> sigma_aT = half_cauchy_rng(0.01);
  real alpha_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_aG);
  real alpha_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_aT);

  real beta = normal_rng(3.0, 1.5);
  real<lower=0> sigma_bG = half_cauchy_rng(0.5);
  real<lower=0> sigma_bT = half_cauchy_rng(0.5);
  real beta_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_bG);
  real beta_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_bT);

  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + beta_T[trt[i]]);
    real delta = alpha + alpha_G[G[i]] + alpha_T[trt[i]];

    theta[i] = inv_logit( gamma * (x[i] - delta) );
    y_sim[i] = binomial_rng(n[i], theta[i]);
  }

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(beta + beta_G[i] + beta_T[j]);
      real mu_a = alpha + alpha_G[i] + alpha_T[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch041-Freaky Screwdriver}
dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                         G = as.integer(age_group), 
                         trt = as.integer(trial),
                         x = x, n = n)) 

m5_2_1 <- sampling(av_iter2_sim, data = dat, 
                   chains = 1, cores = 1, iter = 3000, warmup = 1000,
                   algorithm = "Fixed_param", refresh = 0)
prior5_2_1 <- extract(m5_2_1)
```

### Prior Checks {#iter2-prior-check}

```{r ch041-Surreal Comic}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$pss[,i,2], breaks = 50, probability = TRUE)
}
```


```{r ch041-Severe Kangaroo}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(prior5_2_1$jnd[,i,2], breaks = 50, probability = TRUE)
}
```

```{r ch041-Ivory Brutal Ray}
apply(prior5_2_1$jnd, c(2,3), quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>% round(3)
```

### Configure algorithm {#iter2-config-algo}

### Fit simulated ensemble {#iter2-fit-sim}

```{r ch041-Cosmic Lone}
n_sims <- length(prior5_2_1$alpha)
n_obs <- length(dat$x)
idx <- sample(1:n_sims, n_obs, replace = TRUE)
a <- prior5_2_1$alpha[idx]
b <- prior5_2_1$beta[idx]
probs <- logistic(b * (dat$x - a))
sim_k <- rbinom(n_obs, dat$n, probs)
```


```{stan ch041-Homeless Albatross, output.var="av_iter2_sim_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real aG[N_G];
  real aT[N_T];

  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  real bG[N_G];
  real bT[N_T];
}
model {
  vector[N] theta;

  a  ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  sd_aG ~ cauchy(0, 0.01);
  sd_aT ~ cauchy(0, 0.01);

  b  ~ normal(3.0, 1.5);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch041-Western Weeknight}
sim_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = sim_k)) 

m5_2_2 <- sampling(av_iter2_sim_fit, data = sim_dat, 
                   chains = 10, cores = 10, 
                   iter = 2500, warmup = 2000,
                   refresh = 0, seed = 1)
```


### Algorithmic calibration {#iter2-algo-calibration}

```{r ch041-Intense Canal, message=TRUE}
check_hmc_diagnostics(m5_2_2)
```

Let's see if we can do a little better by reparameterizing the model, and tuning the algorithm a bit. As I'll discuss more in the [model checking](#model-checking) section, we can use the non-centered parameterization of the normal and Cauchy distributions to make it easier for the Hamiltonian Monte Carlo algorithm to explore the posterior. Additionally Stan suggests increasing the `adapt_delta` parameter to remove divergences, so we will do that. Finally, to take care of the message about R-hat and effective sample sizes, I will run the chains for more iterations.

```{stan ch041-Stormy Mountain, output.var="av_iter2_sim_fit_nc"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  real sd_aG;
  real sd_aT;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  real sd_bG;
  real sd_bT;
  
  a = a_raw * 0.05;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  
  b = 3.0 + b_raw * 1.5;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r ch041-Pink Morning}
keep_pars <- c(
  "a", "b",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT"
)

m5_2_3 <- sampling(av_iter2_sim_fit_nc, data = sim_dat, 
                   chains = 4, cores = 4,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   seed = 2)
```

```{r ch041-Dusty Ray, message=TRUE}
check_hmc_diagnostics(m5_2_3)
```

Excellent! Now only about half a percent of the transitions are divergent, and there are no longer any warnings about the R-hat statistic.

### Inferential Calibration {#iter2-inferential-calibration}

### Fit Observation {#iter2-fit-obs}

```{stan ch041-Temple Husky, output.var="av_iter2_obs_fit"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  real sd_aG;
  real sd_aT;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  real sd_bG;
  real sd_bT;
  
  a = a_raw * 0.05;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  
  b = 3.0 + b_raw * 1.5;
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    y_post_pred[i] = binomial_rng(n[i], inv_logit(mu_b * (x[i] - mu_a)));
  }
}
```


```{r ch041-Empty Steel}
obs_dat <- with(av_dat, list(N = N, N_G = N_G, N_T = N_T, 
                             G = as.integer(age_group), 
                             trt = as.integer(trial),
                             x = x, n = n, k = k)) 

keep_pars <- c(
  "a", "b",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT",
  "y_post_pred"
)

m5_2_4 <- sampling(av_iter2_obs_fit, data = obs_dat, 
                   chains = 4, cores = 4,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   seed = 2)
```


### Diagnose posterior fit {#iter2-diagnose-post}

```{r ch041-Hungry Wrench, message=TRUE}
check_hmc_diagnostics(m5_2_4)
```

It's looking alright!

```{r ch041-Green Burst}
stan_summary(m5_2_4, pars = c("a", "aG", "aT"))
stan_summary(m5_2_4, pars = c("b", "bG", "bT"))
stan_summary(m5_2_4, pars = c("pss"))
stan_summary(m5_2_4, pars = c("jnd"))
```


### Posterior retrodictive checks {#iter2-post-retro}

```{r ch041-Purple Clown}
post5_2_4 <- extract(m5_2_4)
post5_2_4_k_pred <- t(apply(post5_2_4$y_post_pred, 2, quantile,
                            probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_2_4$y_post_pred), 1)

m5_2_4_pred <- cbind(post5_2_4_k_pred,
                 post_mean = colMeans(post5_2_4$y_post_pred),
                 post_rand = post5_2_4$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial, age_group = av_dat$age_group) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r ch041-Sleepy Roadrunner}
m5_2_4_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_grid(age_group ~ trial)
```


The retrodictive data are matching well with the observed data, which means that we are getting closer to a model that we can use for inferences.


```{r ch041-Steady Canal}
plot_pf <- function(n, post, age_group, trt) {
  n_smp <- 100
  idx <- sample(1:length(post$a), n_smp, replace = TRUE)
  
  alpha <- with(post, a[idx] + aG[idx, age_group] + aT[idx, trt])
  beta <- with(post, exp(b[idx] + bG[idx, age_group] + bT[idx, trt]))
  
  p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
    ggplot(aes(x, y)) +
    scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
    scale_y_continuous(breaks = c(0, 0.5, 1))
  for (i in 1:n_smp) {
    p <- p + geom_line(stat = "function", fun = fn, 
                       args = list(a = alpha[i],
                                   b = beta[i]),
                       alpha = 0.05)
  }
  p
}
```


```{r ch041-Furious Jazz}
ypre <- plot_pf(100, post5_2_4, 1, 1)
ypos <- plot_pf(100, post5_2_4, 1, 2)
mpre <- plot_pf(100, post5_2_4, 2, 1)
mpos <- plot_pf(100, post5_2_4, 2, 2)
opre <- plot_pf(100, post5_2_4, 3, 1)
opos <- plot_pf(100, post5_2_4, 3, 2)

(ypre + ypos) / (mpre + mpos) / (opre + opos)
```


It's difficult to determine from this graph if there any difference between the age groups. Looking at the density plot of the PSS and JND across the different conditions paints a much clearer image.


```{r ch041-Discarded Torpedo}
age_trt <- expand.grid(a = 1:3, t = 1:2)

dat_pssjnd <- lapply(1:nrow(age_trt), function(i) {
  a <- age_trt$a[i]
  t <- age_trt$t[i]
  tibble(PSS = post5_2_4$pss[,a,t],
         JND = post5_2_4$jnd[,a,t],
         a = a,
         t = t)
}) %>% do.call(what = bind_rows) %>%
  mutate(a = factor(a, levels = 1:3, labels = levels(av_dat$age_group)),
         t = factor(t, levels = 1:2, labels = levels(av_dat$trial))) %>%
  rename(age_group = a, trial = t) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds")

dat_pssjnd %>%
  ggplot(aes(Seconds, fill = trial)) +
  geom_density() +
  scale_fill_manual("trial",
                    values = c(rgb(29/255, 149/255, 219/255, 0.5),
                               rgb(143/255, 19/255, 19/255, 0.5))) +
  facet_grid(age_group ~ Name, scales = "free_x")
```


Using ocular analysis^[Often referred to in the non-sciences as eyeballs] we can see that recalibration has a significant affect on the just noticeable difference for each age group; specifically recalibration heightens temporal sensitivity and thus reduces the just noticeable difference. Comparing between age groups, the young and middle age groups are very similar in both the pre- and post-adaptation trials, and temporal sensitivity is lower in the older age group.

The point of subjective simultaneity between trials is not as well separated, but the model still consistently estimates that the subjects will perceive simultaneity at a value closer to zero post-adaptation.

```{r ch042-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(patchwork)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


## Iteration 3 (The one for Me) {#iter3}

In this iteration of the model building process, we are going to add the individual subjects into the multilevel model, and because this is a simple addition, we are going to skip the prior predictive simulations.

### Model Development {#iter3-model-dev}


```{stan ch042-Forsaken Confidential Ostrich, output.var="av_iter3_obs_fit", echo=TRUE}
data {
  int N;
  int N_G;
  int N_T;
  int N_S;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
  int S[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  real<lower=machine_precision(),upper=pi()/2> aS_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;
  vector[N_S] aS_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  real<lower=machine_precision(),upper=pi()/2> bS_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
  vector[N_S] bS_raw;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  vector[N_S] aS;
  real sd_aG;
  real sd_aT;
  real sd_aS;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  vector[N_S] bS;
  real sd_bG;
  real sd_bT;
  real sd_bS;
  
  // Z * sigma ~ N(0, sigma^2)
  a = a_raw * 0.05;
  
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  sd_aS = 0.05 * tan(aS_unif);
  
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  aS = aS_raw * sd_aS;
  
  // mu + Z * sigma ~ N(mu, sigma^2)
  b = 3.0 + b_raw * 1.5;
  
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  sd_bS = 0.5 * tan(bS_unif);
  
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
  bS = bS_raw * sd_bS;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();
  aS_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();
  bS_raw ~ std_normal();

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]] + bS[S[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]] + aS[S[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }

  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]] + bS[S[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]] + aS[S[i]];
    real theta = inv_logit(mu_b * (x[i] - mu_a));
    y_post_pred[i] = binomial_rng(n[i], theta);
  }
}
```


```{r ch042-Quality Lama}
obs_dat <- with(av_dat, list(
  N = N,
  N_G = N_G,
  N_T = N_T,
  N_S = N_S,
  x = x,
  k = k,
  n = n,
  G = as.integer(av_dat$age_group),
  trt = as.integer(av_dat$trial),
  S = as.integer(av_dat$sid)
))

keep_pars <- c(
  "a", "b",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "aS", "bS",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT", 
  "sd_aS", "sd_bS",
  "y_post_pred"
)

m5_3_1 <- sampling(av_iter3_obs_fit, data = obs_dat, 
                   chains = 4, cores = 4,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   seed = 3)
```

### Diagnose posterior fit {#iter3-diagnose-post}

```{r ch042-Dreaded Elastic Metaphor, message=TRUE}
check_hmc_diagnostics(m5_3_1)
```


```{r ch042-Anaconda Liquid}
stan_summary(m5_3_1, c("a", "aG", "aT"))
stan_summary(m5_3_1, c("b", "bG", "bT"))
stan_summary(m5_3_1, c("pss"))
stan_summary(m5_3_1, c("jnd"))
```


```{r ch042-Cold Bird}
stan_summary(m5_3_1, pars = paste0("sd_a", c("G", "T", "S")))
stan_summary(m5_3_1, pars = paste0("sd_b", c("G", "T", "S")))
```


The number of effective samples and the R-hat indicate that there is no problem with the posterior samples.

### Posterior retrodictive checks {#iter3-post-retro}

```{r ch042-Eastern Mustard}
post5_3_1 <- extract(m5_3_1)
post5_3_1_k_pred <- t(apply(post5_3_1$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_3_1$y_post_pred), 1)

m5_3_1_pred <- cbind(post5_3_1_k_pred,
                 post_mean = colMeans(post5_3_1$y_post_pred),
                 post_rand = post5_3_1$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial, age_group = av_dat$age_group) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r ch042-Leather Lucky}
m5_3_1_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_grid(age_group ~ trial)
```


```{r ch042-Dreaded Monkey}
plot_pf <- function(n, post, age_group, trt) {
  n_smp <- 100
  idx <- sample(1:length(post$a), n_smp, replace = TRUE)
  
  alpha <- with(post, a[idx] + aG[idx, age_group] + aT[idx, trt])
  beta <- with(post, exp(b[idx] + bG[idx, age_group] + bT[idx, trt]))
  
  p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
    ggplot(aes(x, y)) +
    scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
    scale_y_continuous(breaks = c(0, 0.5, 1))
  for (i in 1:n_smp) {
    p <- p + geom_line(stat = "function", fun = fn, 
                       args = list(a = alpha[i],
                                   b = beta[i]),
                       alpha = 0.05)
  }
  p
}
```


```{r ch042-Forsaken Purple Moose}
ypre <- plot_pf(100, post5_3_1, 1, 1)
ypos <- plot_pf(100, post5_3_1, 1, 2)
mpre <- plot_pf(100, post5_3_1, 2, 1)
mpos <- plot_pf(100, post5_3_1, 2, 2)
opre <- plot_pf(100, post5_3_1, 3, 1)
opos <- plot_pf(100, post5_3_1, 3, 2)

(ypre + ypos) / (mpre + mpos) / (opre + opos)
```


It's difficult to determine from this graph if there any difference between the age groups. Looking at the density plot of the PSS and JND across the different conditions paints a much clearer image.


```{r ch042-Severe Lion}
age_trt <- expand.grid(a = 1:3, t = 1:2)

dat_pssjnd <- lapply(1:nrow(age_trt), function(i) {
  a <- age_trt$a[i]
  t <- age_trt$t[i]
  tibble(PSS = post5_3_1$pss[,a,t],
         JND = post5_3_1$jnd[,a,t],
         a = a,
         t = t)
}) %>% do.call(what = bind_rows) %>%
  mutate(a = factor(a, levels = 1:3, labels = levels(av_dat$age_group)),
         t = factor(t, levels = 1:2, labels = levels(av_dat$trial))) %>%
  rename(age_group = a, trial = t) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds")

dat_pssjnd %>%
  ggplot(aes(Seconds, fill = trial)) +
  geom_density() +
  scale_fill_manual("trial",
                    values = c(rgb(29/255, 149/255, 219/255, 0.5),
                               rgb(143/255, 19/255, 19/255, 0.5))) +
  facet_grid(age_group ~ Name, scales = "free_x")
```

Okay what gives? The differences within and between age groups is not a separated as it was in the previous iteration. This is due to the fact that the previous model averaged over the variation at the subject level. We'll consider the task of making predictions at the different levels in the hierarchical model.

```{r ch043-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(ggplot2)
library(patchwork)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

av_dat <- local({
  dat <- audiovisual_binomial %>%
    filter(trial %in% c("pre", "post1"),
           rid != "av-post1-O-f-CE") %>%
    mutate(x = soa / 1000,
           rid = factor(rid),
           sid = factor(sid),
           trial = factor(trial)) %>%
    as.list()
  dat$N <- length(dat$x)
  dat$N_G <- length(levels(dat$age_group))
  dat$N_S <- length(levels(dat$sid))
  dat$N_T <- length(levels(dat$trial))
  dat
})
```


## Iteration 4 (What's one more?) {#iter4}

We now have a model that works well for the audiovisual data, but there are still two other data sets that we can apply the model to. Additionally there is one more modification that we can make to the model that reflects a real world problem - lapses in judgment.

### Conceptual analysis {#iter4-conceptual}

A lapse in judgment can happen for any reason, and is assumed to be random and independent of other lapses. They can come in the form of the subject accidentally blinking during the presentation of a visual stimulus, or unintentionally pressing the wrong button to respond. Whatever the case is, lapses can have a significant affect on the resulting psychometric function.

### Construct summary statistics {#iter4-summary-stats}

We will continue to use the posterior density of the PSS and JND as summary statistics, but the way we calculate them will change as a result of the change in the model in the next section.

### Model Development {#iter4-model-dev}

Lapses can be modeled as occurring independently at some fixed rate. Fundamentally this means that the underlying performance function, $F$, is bounded by some lower and upper lapse rate. This manifests as a scaling and translation of $F$. For a given lower and upper lapse rate $\lambda$ and $\gamma$, the performance function $\Psi$ is 

$$
\Psi(x; \alpha, \beta, \lambda, \gamma) = \lambda + (1 - \lambda - \gamma) F(x; \alpha, \beta)
$$


```{r ch043-plot-pf-with-lapse, fig.cap="Psychometric function with lower and upper performance bounds."}
fn_lapse <- function(x, a=0, b=1, l=0, g=0) l + (1 - l - g) * fn(x, a, b)

ggplot(tibble(x = c(-4, 6), y = c(0, 1)), aes(x, y)) +
  geom_hline(yintercept = 0, lwd = 0.5) +
  geom_vline(xintercept = 0, lwd = 0.5) +
  stat_function(fun = fn_lapse, 
                args = list(a = 1), 
                size = 1, alpha = 0.5,
                color = "steelblue") +
  stat_function(fun = fn_lapse, 
                args = list(a = 1, l=0.05, g=0.11), 
                size = 1.5, 
                color = "orangered") +
  geom_hline(yintercept = 0.05, linetype="dashed") +
  geom_hline(yintercept = 1-0.11, linetype="dashed") +
  annotate("segment", 
           x = -2, xend = -1, 
           y = 0.45, yend = fn_lapse(-1, a=1, l=0.05, g=0.11), 
           color = "gray32", arrow = arrow(type = "closed")) +
  annotate("label", x = -2, y = 0.45, label = "Î¨(x)", vjust = 0, hjust = 1) +
  annotate("segment", x = -2, xend = -1.5, 
           y = 0.25, yend = plogis(-1.5, 1), color = "gray32",
           arrow = arrow(type = "closed")) +
  annotate("label", x = -2, y = 0.25, label = "F(x)", vjust = 0, hjust = 1) +
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) +
  labs(x = "Stimulus Intensity", y = "Probability",
       title = "Psychometric function with lower and upper performance bounds")
```


In certain psychometric experiments, $\lambda$ is interpreted as the lower performance bound or the guessing rate. For example, in certain 2-alternative forced choice (2-AFC) tasks, subjects are asked to respond which of two masses is heavier, and the correctness of their response is recorded. When the masses are the same, the subject can do no better than random guessing. In this task, the lower performance bound is assumed to be 50% as their guess is split between two choices. As the absolute difference in mass grows, the subject's correctness rate increases, though lapses can still happen. In this scenario, $\lambda$ is fixed at $0.5$ and the lapse rate $\gamma$ is a parameter in the model.

Our data does not explicitly record correctness, so we do not give $\lambda$ the interpretation of a guessing rate. Since we are recording proportion of positive responses, we instead treat $\lambda$ and $\gamma$ as lapse rates for negative and positive SOAs. But why should we treat the lapse rates separately? A lapse in judgment can occur independently of the SOA, so $\lambda$ and $\gamma$ should be the same no matter what. With this assumption in mind, we throw away $\gamma$ and assume that the lower and upper performance bounds are restricted by the same amount. I.e.


$$
\Psi(x; \alpha, \beta, \lambda) = \lambda + (1 - 2\lambda) F(x; \alpha, \beta)
$$


While we're throwing in lapse rates, let's also ask the question if different age groups have different lapse rates. To answer this (or rather have our model answer this), we include the new parameter $\lambda_{G[i]}$ into the model so that we get an estimated lapse rate for each age group.

We assume that lapses in judgment are rare, and we know that the rate (or probability of a lapse) is bounded in the interval $[0, 1]$. Because of this, we put a $\mathrm{Beta(4, 96)}$ prior on $\lambda$ which *a priori* puts 99% of the weight below $0.1$ and an expected lapse rate of $0.04$.

We could also set up our model so that information about the lapse rate is shared between age groups (i.e. hierarchical), but we'll leave that as an exercise for the reader.

### Fit the model {#iter4-fit-obs}



```{stan ch043-Eager Modern Parachute, output.var="av_iter4_obs_fit"}
functions {
  real inv_Psi(real p, real a, real b, real l) {
    return logit((p - l) / (1 - 2 * l)) / b + a;
  }
}
data {
  int N;
  int N_G;
  int N_T;
  int N_S;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
  int S[N];
}
parameters {
  real a_raw;
  real<lower=machine_precision(),upper=pi()/2> aG_unif;
  real<lower=machine_precision(),upper=pi()/2> aT_unif;
  real<lower=machine_precision(),upper=pi()/2> aS_unif;
  vector[N_G] aG_raw;
  vector[N_T] aT_raw;
  vector[N_S] aS_raw;

  real b_raw;
  real<lower=machine_precision(),upper=pi()/2> bG_unif;
  real<lower=machine_precision(),upper=pi()/2> bT_unif;
  real<lower=machine_precision(),upper=pi()/2> bS_unif;
  vector[N_G] bG_raw;
  vector[N_T] bT_raw;
  vector[N_S] bS_raw;
  
  vector[N_G] lG;
}
transformed parameters {
  real a;
  vector[N_G] aG;
  vector[N_T] aT;
  vector[N_S] aS;
  real sd_aG;
  real sd_aT;
  real sd_aS;
  
  real b;
  vector[N_G] bG;
  vector[N_T] bT;
  vector[N_S] bS;
  real sd_bG;
  real sd_bT;
  real sd_bS;
  
  // Z * sigma ~ N(0, sigma^2)
  a = a_raw * 0.05;
  
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_aG = 0.01 * tan(aG_unif);
  sd_aT = 0.01 * tan(aT_unif);
  sd_aS = 0.05 * tan(aS_unif);
  
  aG = aG_raw * sd_aG;
  aT = aT_raw * sd_aT;
  aS = aS_raw * sd_aS;
  
  // mu + Z * sigma ~ N(mu, sigma^2)
  b = 3.0 + b_raw * 1.5;
  
  // mu + tau * tan(U) ~ cauchy(mu, tau)
  sd_bG = 0.5 * tan(bG_unif);
  sd_bT = 0.5 * tan(bT_unif);
  sd_bS = 0.5 * tan(bS_unif);
  
  bG = bG_raw * sd_bG;
  bT = bT_raw * sd_bT;
  bS = bS_raw * sd_bS;
}
model {
  vector[N] theta;

  a_raw ~ std_normal();
  aG_raw ~ std_normal();
  aT_raw ~ std_normal();
  aS_raw ~ std_normal();

  b_raw ~ std_normal();
  bG_raw ~ std_normal();
  bT_raw ~ std_normal();
  bS_raw ~ std_normal();
  
  lG ~ beta(4, 96);

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]] + bS[S[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]] + aS[S[i]];
    theta[i] = lG[G[i]] + (1 - 2*lG[G[i]]) * inv_logit(mu_b * (x[i] - mu_a));
  }

  k ~ binomial(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;

  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      real mu_l = lG[i];
      pss[i, j] = inv_Psi(0.50, mu_a, mu_b, mu_l);
      jnd[i, j] = inv_Psi(0.84, mu_a, mu_b, mu_l) - pss[i, j];
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]] + bS[S[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]] + aS[S[i]];
    real mu_l = lG[G[i]];
    real theta = mu_l + (1 - 2*mu_l) * inv_logit(mu_b * (x[i] - mu_a));
    y_post_pred[i] = binomial_rng(n[i], theta);
  }
}
```


```{r ch043-Unique Drill}
obs_dat <- with(av_dat, list(
  N = N,
  N_G = N_G,
  N_T = N_T,
  N_S = N_S,
  x = x,
  k = k,
  n = n,
  G = as.integer(av_dat$age_group),
  trt = as.integer(av_dat$trial),
  S = as.integer(av_dat$sid)
))

keep_pars <- c(
  "a", "b", "lG",
  "pss", "jnd",
  "aG", "bG",
  "aT", "bT",
  "aS", "bS",
  "sd_aG", "sd_bG", 
  "sd_aT", "sd_bT", 
  "sd_aS", "sd_bS",
  "y_post_pred"
)

n_chains <- 4L

init5_4_1 <- replicate(n_chains, list(
  a_raw = rnorm(1),
  aG_raw = rnorm(av_dat$N_G, 0, 0.5),
  aT_raw = rnorm(av_dat$N_T, 0, 0.5),
  aS_raw = rnorm(av_dat$N_S, 0, 0.5),
  aG_unif = runif(1, 0, 0.75),
  aT_unif = runif(1, 0, 0.75),
  aS_unif = runif(1, 0, 0.75),
  b_raw = rnorm(1),
  bG_raw = rnorm(av_dat$N_G, 0, 0.5),
  bT_raw = rnorm(av_dat$N_T, 0, 0.5),
  bS_raw = rnorm(av_dat$N_S, 0, 0.5),
  bG_unif = runif(1, 0, 0.75),
  bT_unif = runif(1, 0, 0.75),
  bS_unif = runif(1, 0, 0.75),
  lG = runif(av_dat$N_G, 0, 0.05)),
simplify = FALSE)

m5_4_1 <- sampling(av_iter4_obs_fit, data = obs_dat, 
                   chains = n_chains, cores = n_chains,
                   iter = 7000, warmup = 2000,
                   refresh = 0, pars = keep_pars,
                   control = list(adapt_delta = 0.95),
                   init = init5_4_1, seed = 4)
```


```{r ch043-Rich Haystack, message=TRUE}
check_hmc_diagnostics(m5_4_1)
```

### Posterior retrodictive checks {#iter4-post-retro}

```{r ch043-Supersonic Torpedo}
post5_4_1 <- extract(m5_4_1)
post5_4_1_k_pred <- t(apply(post5_4_1$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

idx <- sample(1:nrow(post5_4_1$y_post_pred), 1)

m5_4_1_pred <- cbind(post5_4_1_k_pred,
                 post_mean = colMeans(post5_4_1$y_post_pred),
                 post_rand = post5_4_1$y_post_pred[idx,]) %>%
  sweep(1, obs_dat$n, FUN = "/") %>%
  bind_cols(obs_dat, trial = av_dat$trial, age_group = av_dat$age_group) %>%
  select(-N) %>%
  mutate(p = k / n)
```


```{r ch043-Furious Ninth Xylophone}
m5_4_1_pred %>%
  ggplot(aes(x, p)) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.25)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  geom_jitter(width = 0.0125, height = 0.01,
              col = rgb(123, 28, 212, maxColorValue = 255)) +
  geom_jitter(aes(y = post_rand),
              col = rgb(28, 214, 68, 120, maxColorValue = 255),
              width = 0.0125, height = 0.01) +
  facet_grid(age_group ~ trial)
```


```{r ch043-Aberrant Serpent}
plot_pf <- function(n, post, age_group, trt) {
  n_smp <- 100
  idx <- sample(1:length(post$a), n_smp, replace = TRUE)
  
  alpha <- with(post, a[idx] + aG[idx, age_group] + aT[idx, trt])
  beta <- with(post, exp(b[idx] + bG[idx, age_group] + bT[idx, trt]))
  
  p <- tibble(x = c(-0.5, 0.5), y = c(0, 1)) %>%
    ggplot(aes(x, y)) +
    scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
    scale_y_continuous(breaks = c(0, 0.5, 1))
  for (i in 1:n_smp) {
    p <- p + geom_line(stat = "function", fun = fn, 
                       args = list(a = alpha[i],
                                   b = beta[i]),
                       alpha = 0.05)
  }
  p
}
```


```{r ch043-Bulldozer Cold}
ypre <- plot_pf(100, post5_4_1, 1, 1)
ypos <- plot_pf(100, post5_4_1, 1, 2)
mpre <- plot_pf(100, post5_4_1, 2, 1)
mpos <- plot_pf(100, post5_4_1, 2, 2)
opre <- plot_pf(100, post5_4_1, 3, 1)
opos <- plot_pf(100, post5_4_1, 3, 2)

(ypre + ypos) / (mpre + mpos) / (opre + opos)
```


```{r ch043-Discarded Firecracker}
age_trt <- expand.grid(a = 1:3, t = 1:2)

dat_pssjnd <- lapply(1:nrow(age_trt), function(i) {
  a <- age_trt$a[i]
  t <- age_trt$t[i]
  tibble(PSS = post5_4_1$pss[,a,t],
         JND = post5_4_1$jnd[,a,t],
         a = a,
         t = t)
}) %>% do.call(what = bind_rows) %>%
  mutate(a = factor(a, levels = 1:3, labels = levels(av_dat$age_group)),
         t = factor(t, levels = 1:2, labels = levels(av_dat$trial))) %>%
  rename(age_group = a, trial = t) %>%
  tidyr::pivot_longer(c("PSS", "JND"), names_to = "Name", values_to = "Seconds")

dat_pssjnd %>%
  ggplot(aes(Seconds, fill = trial)) +
  geom_density() +
  scale_fill_manual("trial",
                    values = c(rgb(29/255, 149/255, 219/255, 0.5),
                               rgb(143/255, 19/255, 19/255, 0.5))) +
  facet_grid(age_group ~ Name, scales = "free_x")
```


# Model Checking {#model-checking}

- The problem of simulating multivariate data with arbitrary marginal distributions
- Copula approach
  - Nonlinear transformation that invalidates the correlation structure
- Kendall and Spearman matching
  - Nearest Positive Semidefinite correlation matrix
    - Semidefinte Programming (ProxSDP.jl)
    - https://arxiv.org/abs/1810.05231
    - Qi and Sun 2006 (quadratically convergent method)
- Pearson matching
  - Chen 2001 (NORTARA)
  - Xiao, Zhou 2019 (Numeric Approximation)
- Using synthetic data to design experiments
  - Bayesian p-value
  - How much data to notice an effect
  - Bayesian hypothesis testing via predictive performance

> Complementing a calibration free workflow is a pure simulation study that studies the potential problems of a model, and the experimental design it encodes, solely within the assumptions of that model. This is a powerful way both to evaluate experiments before the experiment is build â€“ let alone any data are collected â€“ and to study the behavior of particular modeling techniques in isolation.

# Predictive Inference {#predictive-inference}

- Prior predictive distributions
- Posterior predictive distributions
- Calibrating the model
- Use of synthetic data to assess model properties
- Consequences of using a lapse rate

# Application Results

Objective conclusions

# Discussion

- The history of this project

  - Data Cleaning
    - Data doesn't always come in a nice tidy format
    - Metadata came in the form of directory names
    - Additional metadata (clinical data) came from a separate excel file
      - Ages were provided, but not age groups
    - Actual data were recorded into a Matlab file
    - Not every task had the same column data
    - Not every directory/file name was spelled correctly
      - Adaptation, adapat, 
    - Had to create a unique identifier for each subject
      - subjects with the same initials
      - [task, trial,] age group, gender, and initials
    - Some subjects with two-letter initials, others with three, and yet others that were singly unique
      - JM, JM_F
      - DTF
      - IV_23_, DD_21_
    - Aggregating Bernoulli trials into binomial counts
      - Sufficient statistics
    - Tidying the data
      - Tidy Data by Hadley Wickham
      - each row is an individual observation

  - Developing a model
    - I knew I wanted to apply Bayesian modeling techniques to the data
    - Tried using classical GLM fit first to get a baseline understanding of the data
      - The fact that some estimates for certain subjects failed due to complete separation reinforced my enthusiasm to employ non-classical techniques
    - First model was derived from the psychometric function from Bayesian Cognitive Modeling
      - Used nested loops to iterate over subjects and SOA values
      - Data was messy and the model was hard to build off of
    - Tried `bayes_glm` and `rstanarm`
    - Moved on to multilevel modeling
      - "Statistical Rethinking" by McElreath was my first introduction to Bayesian multilevel modeling
        - His `rethinking` package is too good to be just a book package
      - Tried writing my own package that generates a Stan program based on R formula syntax
        - didn't fully understand the concepts of no-pooling, complete pooling, and partial pooling
        - didn't understand fixed/random effects models
        - Was shot down quickly by some kind folks on Reddit who told me that `brms` and `rstanarm` already did what I was trying to do
          - `brms` and `rstanarm` actually did it better, but I'm not bitter. At all...
        - The fossilized remains of my attempt can be viewed on github
      - Tried using `lme4`, `rstanarm`, and `brms` for random effects modeling
        - was still limited by the lack of control over parameterization and link functions
        - Couldn't easily include a lapse rate parameter into these packages
    - Realized that parameterization can have a significant affect on the efficiency of a model and the inferential power of the estimated parameters
      - When fitting a classical model, there is little difference in estimating `a + bx` vs. `d(x - c)` since the latter can just be expanded as `-cd + dx` which is essentially the same as the first parameterization, but there is a practical difference in the interpretation of the parameters
        - The second parameterization implies that there is a dependence among the parameters that can be factored out
        - In the context of psychometric functions, there is a stronger connection between PSS and `c` and the JND and `d`
        - This reparameterization made it easier to specify priors and also increased the model efficiency
    - Visualization and making shiny plots
      - For the end of STAT 629 - Intro to Bayesian Statistics with Dr. Schissler, I created an interactive graph that lets you select a subject, plot a sample of their estimated psychometric functions, and plot the marginal distributions of SOA values given a probability and probability given an SOA value
    - Still issues with parameterization
      - Treating the condition as an indicator/dummy variable has implications on the variance of predictions
      - Using an indicator variable also introduced an interaction effect into the model that I almost did not account for
      - Switched to index/factor variables helped with model fitting, and interaction effects between factors is handled by creating a new factor that is essentially the cross-product of other factor variables
        - E.g. x=[a, b, c], y=[i, j] --> xy=[ai, aj, bi, bj, ci, cj]
    - One more bout of reparameterization
      - To us, $Z \sim N(0, 1^2);\quad X = 3 + 2Z$ is the same as saying $X \sim N(3, 2^2)$, but to a computer, the process of sampling from $X$ can be more difficult than sampling from $Z$
      - Hierarchical models can benefit greatly from non-centered parameterization
      - Results in more efficient exploration of the posterior, higher number of effected samples, and fewer divergent transitions
    - Side quest: helped explain logistic regression to a fellow who is taking a statistical psychology course in New Zealand
      - He was interested in fitting a 2-AFC model in Julia, which did not provide a link function for such psychometric experiments
      - In the end of our discussion, I wrote my own N-AFC link function that can be dropped in to the `GLM.jl` package

- Model selection is not always the goal
  - Building a model motivated by a set of principals and domain expertise should be the prefereed way
  - Model comparison is important, especially in terms of predictive inference
  - One model doesn't fit all
    - Different models help to answer different questions
      - Mean effect, individual effect, predicitve density, conditional densities, etc
  
