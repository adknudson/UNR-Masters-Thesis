<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Principled Bayesian Workflow | Contributions to Modern Bayesian Multilevel Modeling</title>
  <meta name="description" content="5 Principled Bayesian Workflow | Contributions to Modern Bayesian Multilevel Modeling" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Principled Bayesian Workflow | Contributions to Modern Bayesian Multilevel Modeling" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="adkudson/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Principled Bayesian Workflow | Contributions to Modern Bayesian Multilevel Modeling" />
  
  
  

<meta name="author" content="Alexander Knudson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-modeling.html"/>
<link rel="next" href="model-checking.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="motivating-data.html"><a href="motivating-data.html"><i class="fa fa-check"></i><b>2</b> Background and Motivating Data</a><ul>
<li class="chapter" data-level="2.1" data-path="motivating-data.html"><a href="motivating-data.html#psycho-experiments"><i class="fa fa-check"></i><b>2.1</b> Psychometric Experiments</a></li>
<li class="chapter" data-level="2.2" data-path="motivating-data.html"><a href="motivating-data.html#toj-task"><i class="fa fa-check"></i><b>2.2</b> Temporal Order Judgment Data</a></li>
<li class="chapter" data-level="2.3" data-path="motivating-data.html"><a href="motivating-data.html#data-visualizations-and-quirks"><i class="fa fa-check"></i><b>2.3</b> Data Visualizations and Quirks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>3</b> Background to Modeling</a></li>
<li class="chapter" data-level="4" data-path="bayesian-modeling.html"><a href="bayesian-modeling.html"><i class="fa fa-check"></i><b>4</b> Bayesian Multilevel Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-modeling.html"><a href="bayesian-modeling.html#bayesian-stuff"><i class="fa fa-check"></i><b>4.1</b> Bayesian Stuff</a></li>
<li class="chapter" data-level="4.2" data-path="bayesian-modeling.html"><a href="bayesian-modeling.html#multilevel-modeling-stuff"><i class="fa fa-check"></i><b>4.2</b> Multilevel Modeling Stuff</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>5</b> Principled Bayesian Workflow</a><ul>
<li class="chapter" data-level="5.1" data-path="workflow.html"><a href="workflow.html#pre-model-pre-data"><i class="fa fa-check"></i><b>5.1</b> Pre-Model, Pre-Data</a><ul>
<li class="chapter" data-level="5.1.1" data-path="workflow.html"><a href="workflow.html#conceptual-analysis"><i class="fa fa-check"></i><b>5.1.1</b> Conceptual analysis</a></li>
<li class="chapter" data-level="5.1.2" data-path="workflow.html"><a href="workflow.html#define-observational-space"><i class="fa fa-check"></i><b>5.1.2</b> Define observational space</a></li>
<li class="chapter" data-level="5.1.3" data-path="workflow.html"><a href="workflow.html#construct-summary-statistics"><i class="fa fa-check"></i><b>5.1.3</b> Construct summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="workflow.html"><a href="workflow.html#post-model-pre-data"><i class="fa fa-check"></i><b>5.2</b> Post-Model, Pre-Data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="workflow.html"><a href="workflow.html#model-development"><i class="fa fa-check"></i><b>5.2.1</b> Model development</a></li>
<li class="chapter" data-level="5.2.2" data-path="workflow.html"><a href="workflow.html#construct-summary-functions"><i class="fa fa-check"></i><b>5.2.2</b> Construct summary functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="workflow.html"><a href="workflow.html#simulate-bayesian-ensemble"><i class="fa fa-check"></i><b>5.2.3</b> Simulate bayesian ensemble</a></li>
<li class="chapter" data-level="5.2.4" data-path="workflow.html"><a href="workflow.html#prior-checks"><i class="fa fa-check"></i><b>5.2.4</b> Prior checks</a></li>
<li class="chapter" data-level="5.2.5" data-path="workflow.html"><a href="workflow.html#configure-algorithm"><i class="fa fa-check"></i><b>5.2.5</b> Configure algorithm</a></li>
<li class="chapter" data-level="5.2.6" data-path="workflow.html"><a href="workflow.html#fit-simulated-ensemble"><i class="fa fa-check"></i><b>5.2.6</b> Fit simulated ensemble</a></li>
<li class="chapter" data-level="5.2.7" data-path="workflow.html"><a href="workflow.html#algorithmic-calibration"><i class="fa fa-check"></i><b>5.2.7</b> Algorithmic calibration</a></li>
<li class="chapter" data-level="5.2.8" data-path="workflow.html"><a href="workflow.html#inferential-calibration"><i class="fa fa-check"></i><b>5.2.8</b> Inferential calibration</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="workflow.html"><a href="workflow.html#post-model-post-data"><i class="fa fa-check"></i><b>5.3</b> Post-Model, Post-Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="workflow.html"><a href="workflow.html#fit-observation"><i class="fa fa-check"></i><b>5.3.1</b> Fit observation</a></li>
<li class="chapter" data-level="5.3.2" data-path="workflow.html"><a href="workflow.html#diagnose-posterior-fit"><i class="fa fa-check"></i><b>5.3.2</b> Diagnose posterior fit</a></li>
<li class="chapter" data-level="5.3.3" data-path="workflow.html"><a href="workflow.html#posterior-retrodictive-checks"><i class="fa fa-check"></i><b>5.3.3</b> Posterior retrodictive checks</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="workflow.html"><a href="workflow.html#iteration-2-electric-boogaloo"><i class="fa fa-check"></i><b>5.4</b> Iteration 2 (Electric Boogaloo)</a><ul>
<li class="chapter" data-level="5.4.1" data-path="workflow.html"><a href="workflow.html#model-development"><i class="fa fa-check"></i><b>5.4.1</b> Model Development</a></li>
<li class="chapter" data-level="5.4.2" data-path="workflow.html"><a href="workflow.html#simulate-bayesian-ensemble"><i class="fa fa-check"></i><b>5.4.2</b> Simulate bayesian ensemble</a></li>
<li class="chapter" data-level="5.4.3" data-path="workflow.html"><a href="workflow.html#prior-checks"><i class="fa fa-check"></i><b>5.4.3</b> Prior Checks</a></li>
<li class="chapter" data-level="5.4.4" data-path="workflow.html"><a href="workflow.html#configure-algorithm"><i class="fa fa-check"></i><b>5.4.4</b> Configure algorithm</a></li>
<li class="chapter" data-level="5.4.5" data-path="workflow.html"><a href="workflow.html#fit-simulated-ensemble"><i class="fa fa-check"></i><b>5.4.5</b> Fit simulated ensemble</a></li>
<li class="chapter" data-level="5.4.6" data-path="workflow.html"><a href="workflow.html#algorithmic-calibration"><i class="fa fa-check"></i><b>5.4.6</b> Algorithmic calibration</a></li>
<li class="chapter" data-level="5.4.7" data-path="workflow.html"><a href="workflow.html#inferential-calibration"><i class="fa fa-check"></i><b>5.4.7</b> Inferential Calibration</a></li>
<li class="chapter" data-level="5.4.8" data-path="workflow.html"><a href="workflow.html#fit-observation"><i class="fa fa-check"></i><b>5.4.8</b> Fit Observation</a></li>
<li class="chapter" data-level="5.4.9" data-path="workflow.html"><a href="workflow.html#diagnose-posterior-fit"><i class="fa fa-check"></i><b>5.4.9</b> Diagnose posterior fit</a></li>
<li class="chapter" data-level="5.4.10" data-path="workflow.html"><a href="workflow.html#posterior-retrodictive-checks"><i class="fa fa-check"></i><b>5.4.10</b> Posterior retrodictive checks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>6</b> Model Checking</a></li>
<li class="chapter" data-level="7" data-path="predictive-inference.html"><a href="predictive-inference.html"><i class="fa fa-check"></i><b>7</b> Predictive Inference</a></li>
<li class="chapter" data-level="8" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>8</b> Results</a></li>
<li class="chapter" data-level="9" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>9</b> Discussion</a></li>
<li class="chapter" data-level="10" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>10</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="supplementary-code.html"><a href="supplementary-code.html"><i class="fa fa-check"></i><b>A</b> Supplementary Code</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Contributions to Modern Bayesian Multilevel Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="workflow" class="section level1">
<h1><span class="header-section-number">5</span> Principled Bayesian Workflow</h1>
<p>There are many great resources out there<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> for following along with an analysis of some data or problem, and much more is the abundance of tips, tricks, techniques, and testimonies to good modeling practices. The problem is that many of these prescriptions are given without context for when they are appropriate to be taken. According to <span class="citation">Betancourt (<a href="#ref-betancourt2020" role="doc-biblioref">2020</a>)</span>, this leaves “practitioners to piece together their own model building workflows from potentially incomplete or even inconsistent heuristics.” The concept of a principled workflow is that for any given problem, there is not, nor should there be, a default set of steps to take to get from data exploration to predictive inferences. Rather great consideration must be given to domain expertise and the questions that one is trying to answer with the data.</p>
<p>Since everyone asks different questions, the value of a model is not in how well it ticks the boxes of goodness-of-fit checks, but in consistent it is with domain expertise and its ability to answer the unique set of questions. Betancourt suggests answering four questions to evaluate a model by:</p>
<ol style="list-style-type: decimal">
<li>Domain Expertise Consistency - Is our model consistent with our domain expertise?</li>
<li>Computational Faithfulness - Will our computational tools be sufficient to accurately fit our posteriors?</li>
<li>Inferential Adequacy - Will our inferences provide enough information to answer our questions?</li>
<li>Model Adequacy - Is our model rich enough to capture the relevant structure of the true data generating process?</li>
</ol>
<p><br /></p>
<ul>
<li>Scope out your problem</li>
<li>Specify likelihood and priors</li>
<li>check the model with fake data</li>
<li>fit the model to the real data</li>
<li>check diagnostics</li>
<li>graph fit estimates</li>
<li>check predictive posterior</li>
<li>compare models</li>
</ul>
<div id="pre-model-pre-data" class="section level2">
<h2><span class="header-section-number">5.1</span> Pre-Model, Pre-Data</h2>
<p>Wed begin the modeling process by modeling the experiment by the description of how it occurred and how the data were collected. This first part consists of conceptual analysis, defining the observational space, and constructing summary statistics that can help us to identify issues in the model specification.</p>
<div id="conceptual-analysis" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Conceptual analysis</h3>
<p>In section <a href="motivating-data.html#toj-task">2.2</a> we discussed the experimental setup and data collection. To reiterate, subjects are presented with two stimuli separated by some temporal delay, and they are asked to respond as their perception of the temporal order. There are 45 subjects with 15 each in the young, middle, and older age groups. As the SOA becomes larger in the positive direction, we expect subjects to give more “positive” responses, and as the SOA becomes larger in the negative direction, we expect more “negative” responses. By way the experiment and responses are constructed, we would not expect to see a reversal of this trend unless there was an issue with the subject’s understanding of the directions given to them or an error in the recording device.</p>
<p>We also know that after the first experimental block the subjects go through a recalibration period, and repeat the experiment again. We are interested in seeing if the recalibration has an effect on temporal sensitivity and perceptual synchrony, and if the effect is different for each age group.</p>
</div>
<div id="define-observational-space" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Define observational space</h3>
<p>The response that subjects give to a TOJ task is recorded as a zero or a one (see section <a href="motivating-data.html#toj-task">2.2</a>), and their relative performance is determined by the SOA value. Let <span class="math inline">\(y\)</span> represent the binary outcome of a trial and let <span class="math inline">\(x\)</span> be the SOA value.</p>
<p><span class="math display">\[\begin{align*}
y_i &amp;\in \lbrace 0, 1\rbrace \\
x_i &amp;\in \mathbb{R}
\end{align*}\]</span></p>
<p>If the SOA values are fixed like in the audiovisual task, then the responses can be aggregated into binomial counts, <span class="math inline">\(k\)</span>.</p>
<p><span class="math display">\[
k_i, n_i \in \mathbb{Z}_0^+, k_i \le n_i
\]</span></p>
<p>In the above equation, <span class="math inline">\(\mathbb{Z}_0^+\)</span> represents the set of non-negative integers. Notice that the number of trials <span class="math inline">\(n\)</span> has an index variable <span class="math inline">\(i\)</span>. This is because the number of trials per SOA is not fixed between blocks. In the pre-adaptation block, there are five trials per SOA compared to three in the post-adaptation block. So if observation 32 is recorder during a “pre” block, <span class="math inline">\(n_{32} = 5\)</span>, and if observation 1156 is during a “post” block, <span class="math inline">\(n_{1156} = 3\)</span>.</p>
<p>Then we also have the three categorical variables – age group, subject ID, and adaptation. For the first two, we treat them as factor variables. Rather than using one-hot encoding or dummy variables, we leave the age levels as categories and fit a coefficient for each level. Among the benefits of this approach is the ease of interpretation and ease of working with the data programmatically. This is especially true at the subject level. If we used dummy variables for all 45 subjects, we would have 44 different dummy variables to work with, times the number of coefficients that make estimates at the subject level. In the final iteration of our model, this can be as many as <span class="math inline">\(44 \times 4 = 176\)</span> dummy variables for the subject level!</p>
<p>Age groups and individual subjects can be indexed in the same way that the number of trials is accessed. <span class="math inline">\(S_i\)</span> refers to the subject in record <span class="math inline">\(i\)</span>, and similarly <span class="math inline">\(G_i\)</span> refers to the age group of that subject. Observation 63 is for record ID av-post1-M-f-HG, so then <span class="math inline">\(S_{63}\)</span> is M-f-HG and <span class="math inline">\(G_{63}\)</span> is middle_age. Under the hood of R, these factor levels are represented as integers (e.g. middle age group level is stored internally as the number 2).</p>
<p>We treat the pre- and post-adaptation categories as a binary indicator referred to as <span class="math inline">\(trt\)</span> (short for treatment) since there are only two levels in the category. In this setup, a value of 1 indicates a post-adaptation block. We chose this encoding over the reverse because the pre-adaptation block is like the baseline performance, and it seemed more appropriate to interpret the post-adaptation block as turning on some effect.</p>
<p>In the Stan modeling language, data for a binomial model with subject and age group levels and treatment is specified as</p>
<pre><code>data {
  int N;        // Number of observations
  int N_S;      // Number of subject levels
  int N_G;      // Number of age group levels
  int n[N];     // Trials per SOA
  int k[N];     // binomial counts
  vector[N] x;  // SOA values
  int S[N];     // Subject identifier
  int G[N];     // Age group identifier
  int trt[N];   // Treatment indicator
}</code></pre>
</div>
<div id="construct-summary-statistics" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Construct summary statistics</h3>
<p>In order to effectively challenge the validity of a model, we construct a set of summary statistics that help answer the questions of domain expertise consistency and model adequacy. We are studying the affects of age and temporal recalibration through the PSS and JND (see section <a href="motivating-data.html#psycho-experiments">2.1</a>), so it is natural to define summary statistics around these quantities to verify model consistency. Additionally the PSS and JND can be computed regardless of the model parameterization and chosen psychometric function.</p>
<p>By the experimental setup and recording process, it is impossible that a properly conducted block would result in a JND less than 0 (i.e. a non-increasing psychometric function), so that can be a lower limit for its threshold. On the other end it is unlikely that it will be beyond the limits of the SOA values, but even more concrete, it seems unlikely (though not impossible) that the just noticeable difference would be more than a second.</p>
<p>A histogram of computed PSS and JND values will suffice for summary statistics.</p>
<!-- We can further refine the lower bound on the JND if we draw information from other sources. Some studies show that we cannot perceive time differences below 30 ms, and others show that an input lag as small as 100ms can impair a person's typing ability. -->
</div>
</div>
<div id="post-model-pre-data" class="section level2">
<h2><span class="header-section-number">5.2</span> Post-Model, Pre-Data</h2>
<div id="model-development" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Model development</h3>
<p>Talk here about how I select priors for the intercept (PSS) and the slope (JND). Choose a standard deviation for intercept so that <span class="math inline">\(\approx 95\%\)</span> of the values are between <span class="math inline">\(\pm 0.1\)</span></p>
<p><span class="math display">\[
\begin{align*}
\alpha &amp;\sim \mathcal{N}(0, 0.05) \\
\beta &amp;\sim \mathrm{Lognormal}(3.96, 1.2)
\end{align*}
\]</span></p>
<p>Basically if the expected JND is 0.100 (100 ms) and is distributed log-normally, then <span class="math inline">\(\mathrm{logit}(0.84)/jnd\)</span> is also log-normally distributed with mean <span class="math inline">\(\mathrm{logit}(0.84) - log(0.1) \approx 3.96\)</span>.</p>
<p>Choose a standard deviation value so that <span class="math inline">\(\approx 99\%\)</span> of the JND values are less than 0.500.</p>
<p>The distribution of prior psychometric functions now looks like</p>
<div class="figure" style="text-align: center"><span id="fig:prior-pf-plot"></span>
<img src="050-bayesian-workflow_files/figure-html/prior-pf-plot-1.png" alt="Prior distribution of psychometric functions using the priors for slope and intercept." width="70%" />
<p class="caption">
Figure 5.1: Prior distribution of psychometric functions using the priors for slope and intercept.
</p>
</div>
<p>Notice that the family of psychometric functions covers the broad range of possible slopes and intercepts.</p>
<p>We can now extend the Stan program to include the parameters and model.</p>
<pre><code>parameters {
  real alpha;          // Intercept (PSS)
  real&lt;lower=0&gt; beta;  // Slope (logit(0.84) / JND)
}
model {
  alpha ~ normal(0, 0.05);      // Prior for intercept
  beta ~ lognormal(3.96, 1.2);  // Prior for slope
  vector[N] p;                  // Binomial probability
  for (i in 1:N) {
    p[i] = beta * (x[i] - alpha);
  }
  k ~ binomial_logit(n, p); // Observational model
}</code></pre>
</div>
<div id="construct-summary-functions" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Construct summary functions</h3>
<p>NA</p>
</div>
<div id="simulate-bayesian-ensemble" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Simulate bayesian ensemble</h3>
<p>What is the purpose of this step? To make sure that the generating model coupled with the summary stats/functions yields prior estimates that are consistent with domain expertise (see <a href="workflow.html#prior-checks">5.2.4</a>).</p>
</div>
<div id="prior-checks" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Prior checks</h3>
<blockquote>
<p>If the prior predictive checks indicate con ict between the model and our domain expertise then we have to return to step four [(model development)] and refine our model.</p>
</blockquote>
<pre><code>#&gt;    95%    99%  99.9%   100% 
#&gt; 0.2300 0.5512 1.5245 2.9639</code></pre>
<p><img src="050-bayesian-workflow_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /><img src="050-bayesian-workflow_files/figure-html/unnamed-chunk-4-2.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We’re satisfied with the prior coverage of the PSS and JND, so now we can move on to fitting the model to the simulated data.</p>
</div>
<div id="configure-algorithm" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Configure algorithm</h3>
<p>As a default, we will be using the <code>rstan</code> package <span class="citation">(Guo et al. <a href="#ref-R-rstan" role="doc-biblioref">2020</a>)</span>.</p>
</div>
<div id="fit-simulated-ensemble" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Fit simulated ensemble</h3>
</div>
<div id="algorithmic-calibration" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Algorithmic calibration</h3>
<p>Did the algorithm perform correctly? What kind of diagnostics exist for this algorithm?</p>
<pre><code>#&gt; 
#&gt; Divergences:
#&gt; 
#&gt; Tree depth:
#&gt; 
#&gt; Energy:</code></pre>
<pre><code>#&gt;             mean se_mean     sd       2.5%      97.5% n_eff   Rhat
#&gt; alpha    -0.0012  0.0000 0.0030    -0.0071     0.0046  4077 0.9999
#&gt; beta     16.1855  0.0092 0.4140    15.4021    17.0143  2036 1.0006
#&gt; pss      -0.0012  0.0000 0.0030    -0.0071     0.0046  4077 0.9999
#&gt; jnd       0.1025  0.0001 0.0026     0.0975     0.1077  2022 1.0005
#&gt; lp__  -1418.7510  0.0206 0.9309 -1421.1737 -1417.8138  2040 1.0045</code></pre>
<p><img src="050-bayesian-workflow_files/figure-html/unnamed-chunk-10-1.png" width="70%" style="display: block; margin: auto;" /><img src="050-bayesian-workflow_files/figure-html/unnamed-chunk-10-2.png" width="70%" style="display: block; margin: auto;" /></p>
<ul>
<li>Using HMC
<ul>
<li><span class="math inline">\(\hat{R}\)</span></li>
<li>Divergences</li>
<li>Effective sample size</li>
<li>Tail effective sample size</li>
<li>Bulk effective sample size</li>
<li>Bayesian fraction of missing information</li>
</ul></li>
</ul>
<p>Is there anything we can tune during the fitting process that can alleviate algorithmic issues? Or is it a case of Folk Theorem, and we need to adjust the model?</p>
</div>
<div id="inferential-calibration" class="section level3">
<h3><span class="header-section-number">5.2.8</span> Inferential calibration</h3>
<p>Non-identifiable model??</p>
<blockquote>
<p>In either case we might have to return to Step One to consider an improved experimental design or tempered scientific goals. Sometimes we may only need to return to Step Four to incorporate additional domain expertise to improve our inferences.</p>
</blockquote>
</div>
</div>
<div id="post-model-post-data" class="section level2">
<h2><span class="header-section-number">5.3</span> Post-Model, Post-Data</h2>
<div id="fit-observation" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Fit observation</h3>
</div>
<div id="diagnose-posterior-fit" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Diagnose posterior fit</h3>
<blockquote>
<p>If any diagnostics indicate poor performance then not only is our computational method suspect but also our model might not be rich enough to capture the relevant details of the observed data. At the very least we should return to Step Eight and enhance our computational method.</p>
</blockquote>
<pre><code>#&gt; 
#&gt; Divergences:
#&gt; 
#&gt; Tree depth:
#&gt; 
#&gt; Energy:</code></pre>
</div>
<div id="posterior-retrodictive-checks" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Posterior retrodictive checks</h3>
<p>Need an example of using summary stats on posterior retrodictions</p>
<pre><code>#&gt; # A tibble: 1,827 x 11
#&gt;    `1.5%` `5.5%` `50%` `94.5%` `98.5%` post_mean     x     k     n trt       p
#&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt;
#&gt;  1  0      0     0       0       0.333    0.0105 -0.5      0     3 TRUE  0    
#&gt;  2  0      0     0       0       0.333    0.0168 -0.45     0     3 TRUE  0    
#&gt;  3  0      0     0       0.333   0.333    0.0253 -0.4      0     3 TRUE  0    
#&gt;  4  0      0     0       0.333   0.333    0.0393 -0.35     0     3 TRUE  0    
#&gt;  5  0      0     0       0.333   0.333    0.053  -0.3      0     3 TRUE  0    
#&gt;  6  0      0     0       0.333   0.667    0.0802 -0.25     0     3 TRUE  0    
#&gt;  7  0      0     0       0.333   0.667    0.117  -0.2      0     3 TRUE  0    
#&gt;  8  0      0     0       0.667   0.667    0.176  -0.15     0     3 TRUE  0    
#&gt;  9  0      0     0.333   0.667   0.667    0.236  -0.1      0     3 TRUE  0    
#&gt; 10  0      0     0.333   0.667   1        0.321  -0.05     0     3 TRUE  0    
#&gt; 11  0      0     0.333   1       1        0.420   0        1     3 TRUE  0.333
#&gt; 12  0      0     0.667   1       1        0.527   0.05     0     3 TRUE  0    
#&gt; 13  0      0.333 0.667   1       1        0.629   0.1      2     3 TRUE  0.667
#&gt; 14  0      0.333 0.667   1       1        0.723   0.15     3     3 TRUE  1    
#&gt; 15  0.333  0.333 1       1       1        0.800   0.2      3     3 TRUE  1    
#&gt; 16  0.333  0.667 1       1       1        0.854   0.25     3     3 TRUE  1    
#&gt; 17  0.333  0.667 1       1       1        0.904   0.3      3     3 TRUE  1    
#&gt; 18  0.333  0.667 1       1       1        0.931   0.35     3     3 TRUE  1    
#&gt; 19  0.667  0.667 1       1       1        0.953   0.4      3     3 TRUE  1    
#&gt; 20  0.667  0.667 1       1       1        0.97    0.45     3     3 TRUE  1    
#&gt; 21  0.667  0.667 1       1       1        0.981   0.5      3     3 TRUE  1    
#&gt; # … with 1,806 more rows</code></pre>
<p><img src="050-bayesian-workflow_files/figure-html/unnamed-chunk-16-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Oh no! The posterior retrodictions have failed to capture the variation in the observed data. Even though there were no problems in the model fitting process, we did not come up with a model that is complex enough to capture the features of the data. Of course, we intentionally left out the treatment, age group, and subject variables in order to create a baseline model that we can build off of. We will now go through a second iteration of the model starting back at step 4: model development.</p>

</div>
</div>
<div id="iteration-2-electric-boogaloo" class="section level2">
<h2><span class="header-section-number">5.4</span> Iteration 2 (Electric Boogaloo)</h2>
<div id="model-development" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Model Development</h3>
<p>In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.</p>
<p><span class="math display">\[
\begin{align*}
\beta &amp;\sim \mathcal{N}(3.96, 1.2) \\
\beta_G &amp;\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &amp;\sim \mathcal{N}(0, 1) \\
\beta_{TG} &amp;\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\gamma &amp;\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}
\]</span></p>
<p>In the above formulation, <span class="math inline">\(\gamma\)</span> is a log-normal random variable with mean-log <span class="math inline">\(3.96\)</span> and variance-log <span class="math inline">\(1.2^2 + \sigma_{\beta G}^2\)</span> if it’s the pre-adaptation block, and <span class="math inline">\(1.56^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2\)</span> if it’s the post-adaptation block. Values that are negative reduce the slope and increase the JND, and vice versa for positive values.</p>
<p>The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.</p>
<p><span class="math display">\[
\begin{align*}
\alpha &amp;\sim \mathcal{N}(0, 0.05) \\
\alpha_G &amp;\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &amp;\sim \mathcal{N}(0, 0.05) \\
\alpha_{TG} &amp;\sim \mathcal{N}(0, \sigma_{\alpha TG}^2) \\
\delta &amp;\sim \alpha + \alpha_{G} + (\alpha_{T} + \alpha_{TG}) \times trt
\end{align*}
\]</span></p>
<p>The parameters and model of the Stan program is</p>
<pre><code>parameters {
  real alpha;
  real alpha_G[N_G];
  real alpha_T;
  real alpha_TG[N_G];
  
  real beta;
  real beta_G[N_G];
  real beta_T;
  real beta_TG[N_G];
  
  real&lt;lower=machine_precision()&gt; sigma_aG;
  real&lt;lower=machine_precision()&gt; sigma_aTG;
  real&lt;lower=machine_precision()&gt; sigma_bG;
  real&lt;lower=machine_precision()&gt; sigma_bTG;
}

model {
  alpha ~ normal(0, 0.05);
  alpha_G ~ normal(0, sigma_aG);
  alpha_T ~ normal(0, 0.05);
  alpha_TG ~ normal(0, sigma_aTG);
  
  beta ~ normal(3.96, 1.2);
  beta_G ~ normal(0, sigma_bG);
  beta_T ~ normal(0, 1);
  beta_TG ~ normal(0, sigma_bTG);
  
  sigma_aG ~ cauchy(0, 0.1);
  sigma_aTG ~ cauchy(0, 0.1);
  sigma_bG ~ cauchy(0, 1.0);
  sigma_bTG ~ cauchy(0, 1.0);
  
  vector[N] p;
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    p[i] = gamma * (x[i] - delta);
  }
  k ~ binomial_logit(n, p); // Observational model
}</code></pre>
</div>
<div id="simulate-bayesian-ensemble" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Simulate bayesian ensemble</h3>
</div>
<div id="prior-checks" class="section level3">
<h3><span class="header-section-number">5.4.3</span> Prior Checks</h3>
<p><img src="052-bayesian-workflow_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="052-bayesian-workflow_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;" /></p>
<pre><code>#&gt;        
#&gt;          [,1]  [,2]  [,3]
#&gt;   50%   0.032 0.031 0.032
#&gt;   95%   0.168 0.169 0.170
#&gt;   99%   0.320 0.307 0.314
#&gt;   99.9% 0.725 0.752 0.689
#&gt;        
#&gt;          [,1]  [,2]  [,3]
#&gt;   50%   0.031 0.031 0.031
#&gt;   95%   0.171 0.180 0.179
#&gt;   99%   0.348 0.351 0.330
#&gt;   99.9% 0.819 0.762 0.780</code></pre>
</div>
<div id="configure-algorithm" class="section level3">
<h3><span class="header-section-number">5.4.4</span> Configure algorithm</h3>
</div>
<div id="fit-simulated-ensemble" class="section level3">
<h3><span class="header-section-number">5.4.5</span> Fit simulated ensemble</h3>
<p>Since we are dealing with the log-normal distribution, we need to be careful and specifiy initial values for sampling.</p>
</div>
<div id="algorithmic-calibration" class="section level3">
<h3><span class="header-section-number">5.4.6</span> Algorithmic calibration</h3>
<pre><code>#&gt; 
#&gt; Divergences:
#&gt; 
#&gt; Tree depth:
#&gt; 
#&gt; Energy:</code></pre>
<p>Additionally we were given the warning that the Bulk ESS is too low, and that running the chains for more iterations can help. So we do just that, and also increase the adapt delta.</p>
<pre><code>#&gt;                   mean se_mean     sd       2.5%        25%        50%
#&gt; alpha          -0.0011  0.0006 0.0059    -0.0108    -0.0044    -0.0015
#&gt; alpha_T        -0.0012  0.0002 0.0087    -0.0196    -0.0059    -0.0008
#&gt; sigma_aG        0.0045  0.0002 0.0047     0.0002     0.0014     0.0031
#&gt; sigma_aTG       0.0101  0.0004 0.0107     0.0004     0.0032     0.0075
#&gt; alpha_G[1]     -0.0024  0.0006 0.0056    -0.0230    -0.0038    -0.0008
#&gt; alpha_G[2]     -0.0005  0.0005 0.0052    -0.0199    -0.0017     0.0000
#&gt; alpha_G[3]      0.0012  0.0005 0.0056    -0.0177    -0.0006     0.0006
#&gt; alpha_TG[1]     0.0046  0.0002 0.0093    -0.0098    -0.0004     0.0022
#&gt; alpha_TG[2]     0.0044  0.0002 0.0091    -0.0101    -0.0003     0.0021
#&gt; alpha_TG[3]    -0.0084  0.0003 0.0106    -0.0351    -0.0138    -0.0055
#&gt; beta            2.9663  0.0100 0.1575     2.7770     2.9002     2.9398
#&gt; beta_T         -0.0826  0.0073 0.2174    -0.4787    -0.1594    -0.0891
#&gt; sigma_bG        0.1551  0.0126 0.2873     0.0055     0.0303     0.0655
#&gt; sigma_bTG       0.2594  0.0119 0.4132     0.0085     0.0625     0.1337
#&gt; beta_G[1]      -0.0173  0.0105 0.1556    -0.4123    -0.0274     0.0036
#&gt; beta_G[2]      -0.0294  0.0079 0.1548    -0.4139    -0.0424    -0.0022
#&gt; beta_G[3]      -0.0499  0.0095 0.1593    -0.4453    -0.0627    -0.0134
#&gt; beta_TG[1]     -0.0568  0.0075 0.2212    -0.5900    -0.1113    -0.0293
#&gt; beta_TG[2]      0.0543  0.0074 0.2202    -0.4121    -0.0124     0.0330
#&gt; beta_TG[3]     -0.0324  0.0072 0.2180    -0.5525    -0.0898    -0.0120
#&gt; pss_pre[1]     -0.0035  0.0001 0.0046    -0.0136    -0.0062    -0.0031
#&gt; pss_pre[2]     -0.0016  0.0001 0.0042    -0.0100    -0.0042    -0.0016
#&gt; pss_pre[3]      0.0001  0.0001 0.0047    -0.0083    -0.0032    -0.0001
#&gt; pss_post[1]    -0.0001  0.0001 0.0071    -0.0134    -0.0047    -0.0007
#&gt; pss_post[2]     0.0016  0.0001 0.0068    -0.0107    -0.0029     0.0011
#&gt; pss_post[3]    -0.0095  0.0004 0.0089    -0.0285    -0.0154    -0.0086
#&gt; jnd_pre[1]      0.0870  0.0001 0.0044     0.0778     0.0842     0.0872
#&gt; jnd_pre[2]      0.0880  0.0002 0.0044     0.0798     0.0850     0.0880
#&gt; jnd_pre[3]      0.0899  0.0001 0.0044     0.0817     0.0869     0.0897
#&gt; jnd_post[1]     0.1001  0.0001 0.0070     0.0871     0.0953     0.0998
#&gt; jnd_post[2]     0.0907  0.0002 0.0070     0.0770     0.0860     0.0908
#&gt; jnd_post[3]     0.1009  0.0001 0.0070     0.0879     0.0961     0.1008
#&gt; lp__        -1243.3680  0.2765 5.8057 -1255.0649 -1247.0388 -1243.3967
#&gt;                    75%      97.5%   n_eff  Rhat
#&gt; alpha           0.0014     0.0199   99.31 1.040
#&gt; alpha_T         0.0035     0.0157 3099.45 1.000
#&gt; sigma_aG        0.0059     0.0166  381.76 1.014
#&gt; sigma_aTG       0.0136     0.0361  702.28 1.009
#&gt; alpha_G[1]      0.0004     0.0048  100.55 1.046
#&gt; alpha_G[2]      0.0015     0.0086   97.45 1.037
#&gt; alpha_G[3]      0.0033     0.0128  107.69 1.030
#&gt; alpha_TG[1]     0.0084     0.0279 2404.48 1.004
#&gt; alpha_TG[2]     0.0082     0.0270 2453.88 1.003
#&gt; alpha_TG[3]    -0.0006     0.0047  992.01 1.006
#&gt; beta            2.9860     3.3468  248.36 1.021
#&gt; beta_T         -0.0190     0.4171  888.53 1.006
#&gt; sigma_bG        0.1600     0.7986  515.72 1.010
#&gt; sigma_bTG       0.2885     1.2821 1200.36 1.004
#&gt; beta_G[1]       0.0382     0.1792  218.71 1.023
#&gt; beta_G[2]       0.0267     0.1595  385.89 1.017
#&gt; beta_G[3]       0.0102     0.1383  281.83 1.020
#&gt; beta_TG[1]      0.0158     0.3232  862.37 1.006
#&gt; beta_TG[2]      0.1243     0.5070  884.48 1.005
#&gt; beta_TG[3]      0.0346     0.3628  919.51 1.005
#&gt; pss_pre[1]     -0.0004     0.0048 3334.50 1.001
#&gt; pss_pre[2]      0.0012     0.0066 3394.09 1.002
#&gt; pss_pre[3]      0.0030     0.0103 1484.76 1.005
#&gt; pss_post[1]     0.0043     0.0154 4140.14 1.002
#&gt; pss_post[2]     0.0060     0.0162 2982.42 1.003
#&gt; pss_post[3]    -0.0030     0.0056  612.57 1.010
#&gt; jnd_pre[1]      0.0898     0.0953 3914.60 1.001
#&gt; jnd_pre[2]      0.0909     0.0971  830.30 1.003
#&gt; jnd_pre[3]      0.0926     0.0993 2043.18 1.002
#&gt; jnd_post[1]     0.1047     0.1150 5881.39 1.001
#&gt; jnd_post[2]     0.0957     0.1046 1211.42 1.004
#&gt; jnd_post[3]     0.1054     0.1156 6596.14 1.000
#&gt; lp__        -1239.5208 -1231.3856  440.90 1.007</code></pre>
</div>
<div id="inferential-calibration" class="section level3">
<h3><span class="header-section-number">5.4.7</span> Inferential Calibration</h3>
</div>
<div id="fit-observation" class="section level3">
<h3><span class="header-section-number">5.4.8</span> Fit Observation</h3>
</div>
<div id="diagnose-posterior-fit" class="section level3">
<h3><span class="header-section-number">5.4.9</span> Diagnose posterior fit</h3>
</div>
<div id="posterior-retrodictive-checks" class="section level3">
<h3><span class="header-section-number">5.4.10</span> Posterior retrodictive checks</h3>
<pre><code>#&gt; # A tibble: 1,827 x 13
#&gt;    `1.5%` `5.5%` `50%` `94.5%` `98.5%` post_mean   N_G     x     k     n     G
#&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
#&gt;  1      0  0     0.333   0.667       1     0.240     3 -0.5      0     3     2
#&gt;  2      0  0     0.333   0.667       1     0.263     3 -0.45     0     3     2
#&gt;  3      0  0     0.333   0.667       1     0.287     3 -0.4      0     3     2
#&gt;  4      0  0     0.333   0.667       1     0.305     3 -0.35     0     3     2
#&gt;  5      0  0     0.333   0.667       1     0.328     3 -0.3      0     3     2
#&gt;  6      0  0     0.333   0.667       1     0.357     3 -0.25     0     3     2
#&gt;  7      0  0     0.333   0.667       1     0.377     3 -0.2      0     3     2
#&gt;  8      0  0     0.333   1           1     0.401     3 -0.15     0     3     2
#&gt;  9      0  0     0.333   1           1     0.422     3 -0.1      0     3     2
#&gt; 10      0  0     0.333   1           1     0.455     3 -0.05     0     3     2
#&gt; 11      0  0     0.333   1           1     0.478     3  0        1     3     2
#&gt; 12      0  0     0.667   1           1     0.506     3  0.05     0     3     2
#&gt; 13      0  0     0.667   1           1     0.523     3  0.1      2     3     2
#&gt; 14      0  0     0.667   1           1     0.553     3  0.15     3     3     2
#&gt; 15      0  0     0.667   1           1     0.585     3  0.2      3     3     2
#&gt; 16      0  0     0.667   1           1     0.608     3  0.25     3     3     2
#&gt; 17      0  0.333 0.667   1           1     0.630     3  0.3      3     3     2
#&gt; 18      0  0.333 0.667   1           1     0.656     3  0.35     3     3     2
#&gt; 19      0  0.333 0.667   1           1     0.679     3  0.4      3     3     2
#&gt; 20      0  0.333 0.667   1           1     0.699     3  0.45     3     3     2
#&gt; 21      0  0.333 0.667   1           1     0.717     3  0.5      3     3     2
#&gt; # … with 1,806 more rows, and 2 more variables: trt &lt;int&gt;, p &lt;dbl&gt;</code></pre>
<p><img src="052-bayesian-workflow_files/figure-html/unnamed-chunk-17-1.png" width="70%" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-betancourt2020">
<p>Betancourt, Michael. 2020. “Towards a Principled Bayesian Workflow.” <em>Betanalpha</em>. <a href="betanalpha.github.io">betanalpha.github.io</a>.</p>
</div>
<div id="ref-R-rstan">
<p>Guo, Jiqiang, Jonah Gabry, Ben Goodrich, and Sebastian Weber. 2020. <em>Rstan: R Interface to Stan</em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>citation needed<a href="workflow.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-modeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-checking.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/adknudson/thesis/blob/master/050-bayesian-workflow.Rmd",
"text": null
},
"download": ["adknudson-thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toc_depth": 3,
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
