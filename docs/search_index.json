[
["workflow.html", "5 Principled Bayesian Workflow 5.1 Pre-Model, Pre-Data 5.2 Post-Model, Pre-Data 5.3 Post-Model, Post-Data", " 5 Principled Bayesian Workflow There are many great resources out there1 for following along with an analysis of some data or problem, and much more is the abundance of tips, tricks, techniques, and testimonies to good modeling practices. The problem is that many of these prescriptions are given without context for when they are appropriate to be taken. According to Betancourt (2020), this leaves “practitioners to piece together their own model building workflows from potentially incomplete or even inconsistent heuristics.” The concept of a principled workflow is that for any given problem, there is not, nor should there be, a default set of steps to take to get from data exploration to predictive inferences. Rather great consideration must be given to domain expertise and the questions that one is trying to answer with the data. Since everyone asks different questions, the value of a model is not in how well it ticks the boxes of goodness-of-fit checks, but in consistent it is with domain expertise and its ability to answer the unique set of questions. Betancourt suggests answering four questions to evaluate a model by: Domain Expertise Consistency - Is our model consistent with our domain expertise? Computational Faithfulness - Will our computational tools be sufficient to accurately fit our posteriors? Inferential Adequacy - Will our inferences provide enough information to answer our questions? Model Adequacy - Is our model rich enough to capture the relevant structure of the true data generating process? Scope out your problem Specify likelihood and priors check the model with fake data fit the model to the real data check diagnostics graph fit estimates check predictive posterior compare models 5.1 Pre-Model, Pre-Data 5.1.1 Conceptual analysis What are we really trying to accomplish with this experiment? Ideally we would like to answer a grid of questions related to age, task, and temporal recalibration, and how these factors affect perceptual synchrony and temporal sensitivity 5.1.2 Define observational space Given the concept of a psychometric experiment, we can begin to develop a formal mathematical model. The response that subjects give to a TOJ task is recorded as a zero or a one (see section 2.2). The response is dependent on some temporal delay between stimuli, and the delay is small – less than one second. For the audiovisual, visual, and duration tasks, the SOA values are fixed, and multiple trials are presented for each SOA. 5.1.3 Construct summary statistics In order to effectively challenge the validity of a model, we construct a set of summary statistics that help answer questions 1 and 4. We are studying the affects of age and temporal recalibration through the PSS and JND (see section 2.1), and we can put reasonable thresholds on those quantities. 5.2 Post-Model, Pre-Data 5.2.1 Model development Now that the conceptual analysis is complete, we can construct an observational model and a complementary prior model to form a complete Bayesian model. The observational space is the set of responses \\(\\lbrace 0, 1\\rbrace\\) and the SOA values \\(\\mathbb{R}\\). Additionally each experiment has a unique identifier for the task, block, and subject (e.g. av-pre-M-f-DB). This identifier can be broken down to yield categorical variables or identifiers for each of the three, which is useful for models that make inferences at different levels in a hierarchy. As was stated in section 3, a subject’s response is the outcome of a Bernoulli trial parameterized by a probability, \\(\\pi\\), that is dependent on the SOA value and potentially other contextual and biological factors. 5.2.2 Construct summary functions NA 5.2.3 Simulate bayesian ensemble What is the purpose of this step? To make sure that the generating model coupled with the summary stats/functions yields prior estimates that are consistent with domain expertise (see 5.2.4). 5.2.4 Prior checks If the prior predictive checks indicate con ict between the model and our domain expertise then we have to return to step four [(model development)] and refine our model. 5.2.5 Configure algorithm As a default, we will be using the rethinking package (McElreath 2020) which is built on top of rstan (Guo et al. 2020). The rethinking package provides a high-level interface to Stan – a C-compiled Markov chain Monte Carlo (MCMC) sampler – while still being flexible enough to specify complex Bayesian models. m1_0 &lt;- alist( response ~ bernoulli_logit(p), p = a + b * soa, a ~ normal(0, 100), b ~ normal(0, 100) ) 5.2.6 Fit simulated ensemble f1_0 &lt;- ulam(m1_0, data = dat, chains = 8, cores = 8, log_lik = TRUE) 5.2.7 Algorithmic calibration Did the algorithm perform correctly? What kind of diagnostics exist for this algorithm? Using HMC \\(\\hat{R}\\) Divergences Effective sample size Tail effective sample size Bulk effective sample size Bayesian fraction of missing information Is there anything we can tune during the fitting process that can alleviate algorithmic issues? Or is it a case of Folk Theorem, and we need to adjust the model? 5.2.8 Inferential calibration 5.3 Post-Model, Post-Data 5.3.1 Fit observation 5.3.2 Diagnose posterior fit 5.3.3 Posterior retrodictive checks 5.3.4 Celebrate References "]
]
