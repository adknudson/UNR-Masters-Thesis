---
title: "Contributions to Modern Bayesian Multilevel Modeling"
author: "Alexander Knudson"
advisor: "A.G. Schissler"
date: "August, 2020"
site: bookdown::bookdown_site
bibliography: [bibliography.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: adkudson/thesis
---

```{r ch1-setup, include=FALSE}
knitr::write_bib(c('knitr', 'bookdown', 'rstan'), 
                 'packages.bib', width = 80)
```

# Introduction

With the advances in computational power and high-level programming languages like Python, R, and Julia, statistical methods have evolved to be more flexible and expressive. No longer must we be subjugated by p-values and step-wise regression techniques. Gone are the days of using clever modeling techniques to tame misbehaved data. Now is the time for principled and informed decisions to create bespoke models and domain-motivated analyses. We have the shoulders of giants to stand upon and look out at the vast sea of data science.

I want to talk about how the advances in computational power have lead to a sort of mini revolution - resurrection - in statistics where Bayesian modeling has gained an incredible following thanks to projects like Stan. The steady adoption of computer aided statistical workflows also brings the need for multidisciplinary techniques from numerical analysis, probability theory, statistics, computer science, visualizations, and more. And with the age of computers, there is a strong push towards reproducibility. Concepts of modular design, workflows, project history and versioning, virtual environments, and human readable code all contribute to reproducible analyses. And somehow I also want to tie in how data is immutable - raw data should (must) be treated as a constant and unchangeable entity, and merely touching it will cause data mitosis.

## Classical Approaches to Modeling

- Overview of classical modeling methods
  - classical approaches to data analysis usually adhere to the flexibility-interpretability trade-off
  - generally inflexible (parametric) to be more interpretable and computationally easier
  - sometimes a model is too flexible (non-parametric) and loses crucial inferential power
  - sometimes our assumptions about the data are invalid
    - normality, independence, heteroskedacity, etc.
  - often limited when it comes to statistical summaries and confidence intervals

Computational Power 
  --> Importance of reproducibility and workflows
  --> Classical modeling methods 
    --> Problems with classical methods
  --> Proposed solutions

- Solutions or alternatives when classical models fail
  - Bayesian inference is a powerful, descriptive, and flexible modeling framework
  - Bayes theorem is a simple model of incorporating prior information and data to produce a posterior probability or distribution
  - $P(\theta | X) \propto P(X | \theta) * P(\theta)$ or $posterior \propto prior \times likelihood$
    - The prior is some distribution over the parameter space
    - The likelihood is the probability of an outcome in the sample space given a value in the parameter space
    - The posterior is the likelihood of values in the parameter space after observing values from the sample space
  - Bayesian statistics, when described without math, actually feels natural to most people
    - you hear hoof beats, you think horses, not zebras [unless you're in Africa, but that's prior information ;)]
  - The catch is that the model is not complete as written above
  - There is actually a denominator in Bayes' Theorem
    - $P(\theta | X) = \frac{P(X | \theta)\cdot P(\theta)}{\sum_i P(X | \theta_i)} = \frac{P(X | \theta)\cdot P(\theta)}{\int_\Omega P(X | \theta)d\theta}$
    - In general, the denominator is not known, or is not not easy (or possible) to calculate, but it always evaluates to a constant (hence the "proportional to")
    - The denominator acts as a scaling value that forces $P(\theta|X)$ to be a probability distribution (i.e. area under PDF is equal to 1)
    - There are simulation-based techniques that let one approximate the posterior distribution without needing to know the analytic solution to the denominator

## Proposal of New Methods

## Organization

I have organized this thesis as follows. In [Chapter 2](#motivating-data) I introduce the data set that drives the narrative and that motivates the adoption of Bayesian multilevel modeling. In [Chapter 3](#background) there is a review of common approaches approaches to modeling with psychometric data, and the benefits and drawbacks of such techniques. [Chapter 4](#bayesian-modeling) introduces Bayesian hierarchical modeling and programming frameworks for Bayesian inference. In [Chapter 5](#workflow) I describe and work through a principled Bayesian workflow for multilevel modeling. [Chapter 6](#model-checking) goes into more depth on checking the model goodness of fit and model diagnostics in a Bayesian setting. Finally in [Chapter 7](#predictive-inference) I demonstrate how to use the Bayesian model from the principled workflow for predictive inference, and use posterior predictive distributions to plot and compare models.
