```{r ch5_2-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```


## Iteration 2 (Electric Boogaloo)

### Model Development

In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.

$$
\begin{align*}
\beta &\sim \mathcal{N}(2.5, 1^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, 1^2) \\
\beta_{TG} &\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\mu_\beta &\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}
$$

In the above formulation, $\mu_\beta$ is a log-normal random variable with mean-log $2.5$ and variance-log $1^2 + \sigma_{\beta G}^2$ if it's the pre-adaptation block, and $\left(\sqrt{1^2 + 1^2}\right)^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2$ if it's the post-adaptation block. Values that are negative reduce the slope (increase the JND), and values that are positive increase the slope (reduce the JND). 

But wait! this model implies that there is more uncertainty about the post-adaptation trials compared to the baseline trials, and this is not necessarily true. Furthermore, as we'll see in the linear part of model, the intercept, $\alpha$, is no longer the average response probability of the sample, but is instead exclusively the average for the pre-adaptation trials. This may not matter in certain analyses, but one nice property of multilevel models is the separation of population level estimates and group level estimates. So we modify the model for the slope to be:

$$
\begin{align*}
\beta &\sim \mathcal{N}(2.5, 1^2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, \sigma_{\beta T}^2) \\
\mu_\beta &= \exp(\beta + \beta_G + \beta_T)
\end{align*}
$$

Now $\mu_\beta$ is a log-normal random variable with mean-log $2.5$ and variance-log $1^2 + \sigma_{\beta G}^2 + \sigma_{\beta T}^2$, regardless of whether it's the pre-adaptation or the post-adaptation block.

The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.

$$
\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05^2) \\
\alpha_G &\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &\sim \mathcal{N}(0, \sigma_{\alpha T}^2) \\
\mu_\alpha &= \alpha + \alpha_{G} + \alpha_{T}
\end{align*}
$$

The parameters and model of the Stan program is

```
parameters {
  real a;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real bG[N_G];
  real bT[N_T];
  
  real<lower=machine_precision()> sd_aG;
  real<lower=machine_precision()> sd_aT;
  real<lower=machine_precision()> sd_bG;
  real<lower=machine_precision()> sd_bT;
}
model {
  a ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  
  b ~ normal(2.5, 1.0);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  
  sd_aG ~ cauchy(0, 0.05);
  sd_aT ~ cauchy(0, 0.05);
  sd_bG ~ cauchy(0, 0.5);
  sd_bT ~ cauchy(0, 0.5);
  
  vector[N] p;
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[T[i]]);
    real mu_a = a + aG[G[i]] + aT[T[i]];
    p[i] = mu_b * (x[i] - mu_a);
  }
  k ~ binomial_logit(n, p); // Observational model
}
```




###  Simulate bayesian ensemble

```{stan output.var="sim_iter2"}
functions {
  real half_cauchy_rng(real sigma) {
    real u = uniform_rng(0.5, 0.99);
    real y = sigma * tan(pi() * (u - 0.5));
    return y;
  }
}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
generated quantities {
  vector[N] theta;
  int y_sim[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;
  
  real alpha = normal_rng(0, 0.05);
  real<lower=0> sigma_aG = half_cauchy_rng(0.05);
  real<lower=0> sigma_aT = half_cauchy_rng(0.05);
  real alpha_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_aG);
  real alpha_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_aT);
  
  real beta = normal_rng(2.5, 1.0);
  real<lower=0> sigma_bG = half_cauchy_rng(0.5);
  real<lower=0> sigma_bT = half_cauchy_rng(0.5);
  real beta_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_bG);
  real beta_T[N_T] = normal_rng(rep_vector(0, N_T), sigma_bT);
  
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + beta_T[trt[i]]);
    real delta = alpha + alpha_G[G[i]] + alpha_T[trt[i]];
    
    theta[i] = inv_logit( gamma * (x[i] - delta) );
    y_sim[i] = binomial_rng(n[i], theta[i]);
  }
  
  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(beta + beta_G[i] + beta_T[j]);
      real mu_a = alpha + alpha_G[i] + alpha_T[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r}
av_dat <- audiovisual_binomial %>%
  filter(trial %in% c("pre", "post1"),
         rid != "av-post1-O-f-CE") %>%
  mutate(trt = droplevels(trial))

sim_dat2 = list(N = nrow(av_dat),
                N_G = max(as.integer(av_dat$age_group)),
                N_T = max(as.integer(av_dat$trt)),
                n = av_dat$n,
                x = av_dat$soa / 1000,
                G = as.integer(av_dat$age_group),
                trt = as.integer(av_dat$trt))
```


```{r}
sim_fit2 <- sampling(sim_iter2, data = sim_dat2, 
                     algorithm="Fixed_param", refresh = 0)
sim_prior2 <- extract(sim_fit2)
```

### Prior Checks

```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(sim_prior2$pss[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(sim_prior2$pss[,i,2], breaks = 50, probability = TRUE)
}
```


```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(sim_prior2$jnd[,i,1], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(sim_prior2$jnd[,i,2], breaks = 50, probability = TRUE)
}
```

```{r}
apply(sim_prior2$jnd, c(2,3), quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>%  round(3)
```

### Configure algorithm

### Fit simulated ensemble

```{stan output.var="fit_sim_iter2"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  real bG[N_G];
  real bT[N_T];
}
model {
  vector[N] theta;

  a  ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  sd_aG ~ cauchy(2.5, 2.5);
  sd_aT ~ cauchy(2.5, 2.5);
  
  b  ~ normal(2.5, 1.0);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  sd_bG ~ cauchy(2.5, 2.5);
  sd_bT ~ cauchy(2.5, 2.5);
  
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }
  
  k ~ binomial_logit(n, theta);
}
generated quantities {
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;
  
  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }
}
```


```{r}
sim_dat2$k <- sapply(1:ncol(sim_prior2$y_sim), function(i) {
  sim_prior2$y_sim[sample(4000, 1), i]
})
```


```{r}
sim_fit2 <- sampling(fit_sim_iter2, data = sim_dat2, refresh = 1000)
sim_post2 <- extract(sim_fit2)
```


### Algorithmic calibration

```{r}
check_hmc_diagnostics(sim_fit2)
```

Additionally we were given the warning that the Bulk ESS is too low, and that running the chains for more iterations can help. So we do just that, and also increase the adapt delta.

```{r}
sim_fit2 <- sampling(fit_sim_iter2, data = sim_dat2, iter = 4000, warmup = 1000,
                     refresh = 1000, control = list(adapt_delta = 0.95,
                                                    max_treedepth = 12))
sim_post2 <- extract(sim_fit2)
```

```{r}
summary(sim_fit2)$summary %>% round(4)
```

### Inferential Calibration

### Fit Observation

```{stan output.var="fit_obs_iter2"}
data {
  int N;
  int N_G;
  int N_T;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real a;
  real<lower=0> sd_aG;
  real<lower=0> sd_aT;
  real aG[N_G];
  real aT[N_T];
  
  real b;
  real<lower=0> sd_bG;
  real<lower=0> sd_bT;
  real bG[N_G];
  real bT[N_T];
}
model {
  vector[N] theta;

  a  ~ normal(0, 0.05);
  aG ~ normal(0, sd_aG);
  aT ~ normal(0, sd_aT);
  sd_aG ~ cauchy(2.5, 2.5);
  sd_aT ~ cauchy(2.5, 2.5);
  
  b  ~ normal(2.5, 1.0);
  bG ~ normal(0, sd_bG);
  bT ~ normal(0, sd_bT);
  sd_bG ~ cauchy(2.5, 2.5);
  sd_bT ~ cauchy(2.5, 2.5);
  
  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    theta[i] = mu_b * (x[i] - mu_a);
  }
  
  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  matrix[N_G, N_T] pss;
  matrix[N_G, N_T] jnd;
  
  for (i in 1:N_G) {
    for (j in 1:N_T) {
      real mu_b = exp(b + bG[i] + bT[j]);
      real mu_a = a + aG[i] + aT[j];
      pss[i, j] = mu_a;
      jnd[i, j] = logit(0.84) / mu_b;
    }
  }

  for (i in 1:N) {
    real mu_b = exp(b + bG[G[i]] + bT[trt[i]]);
    real mu_a = a + aG[G[i]] + aT[trt[i]];
    y_post_pred[i] = binomial_rng(n[i], inv_logit(mu_b * (x[i] - mu_a)));
  }
}
```




```{r}
obs_dat2 <- list(N = nrow(av_dat),
                 N_G = max(as.integer(av_dat$age_group)),
                 N_T = max(as.integer(av_dat$trt)),
                 x = av_dat$soa / 1000,
                 k = av_dat$k,
                 n = av_dat$n,
                 G = as.integer(av_dat$age_group),
                 trt = as.integer(av_dat$trt))

m2_fit <- sampling(fit_obs_iter2, data = obs_dat2, iter = 5000, warmup = 2500,
                   refresh = 2500, 
                   control = list(adapt_delta=0.95, max_treedepth=12))
```


### Diagnose posterior fit

```{r}
check_hmc_diagnostics(m2_fit)
```

It's looking alright!

### Posterior retrodictive checks

```{r}
m2_post <- extract(m2_fit)
m2_summ <- summary(m2_fit)$summary
```

```{r}
m2_post_k_pred <- t(apply(m2_post$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

m2_pred <- cbind(m2_post_k_pred, post_mean = colMeans(m2_post$y_post_pred)) %>%
  sweep(1, obs_dat2$n, FUN = "/") %>%
  bind_cols(obs_dat2) %>%
  select(-N) %>%
  mutate(p = k / n)

print(m2_pred, n = 21)
```


```{r}
m2_pred %>%
  ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = `1.5%`, ymax = `98.5%`), alpha = 0.05) +
  geom_ribbon(aes(ymin = `5.5%`, ymax = `94.5%`), alpha = 0.15) +
  geom_point(alpha = 0.1) +
  geom_point(aes(y = `50%`), col = "green", alpha = 0.5) +
  facet_grid(G~trt)
```

We're getting closer to an acceptable model, but our model's retrodictions still do not represent the data very well. Specifically the retrodictions are still under-dispersed. 
