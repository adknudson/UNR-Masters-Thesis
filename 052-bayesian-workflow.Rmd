```{r ch5_2-setup, include=FALSE}
library(FangPsychometric)
library(dplyr)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```


## Iteration 2 (Electric Boogaloo)

### Model Development

In this iteration we will now add in the treatment and age group levels. Instead of modeling the prior distribution of the slope as log-normal, we model it as a normal distribution and then take the exponential. This allows us to also model the age group and treatment slopes as normally distributed and with an additive affect.

$$
\begin{align*}
\beta &\sim \mathcal{N}(4, 2) \\
\beta_G &\sim \mathcal{N}(0, \sigma_{\beta G}^2) \\
\beta_T &\sim \mathcal{N}(0, 1) \\
\beta_{TG} &\sim \mathcal{N}(0, \sigma_{\beta TG}^2) \\
\gamma &\sim \exp(\beta + \beta_G + (\beta_T + \beta_{TG})\times trt)
\end{align*}
$$

In the above formulation, $\gamma$ is a log-normal random variable with mean-log $4$ and variance-log $2^2 + \sigma_{\beta G}^2$ if it's the pre-adaptation block, and $1.56^2 + \sigma_{\beta G}^2 + \sigma_{\beta TG}^2$ if it's the post-adaptation block. Values that are negative reduce the slope and increase the JND, and vice versa for positive values.

The intercept term can be specified similarly. Conservatively we choose the prior for the intercepts to be normally distributed with mean 0.

$$
\begin{align*}
\alpha &\sim \mathcal{N}(0, 0.05) \\
\alpha_G &\sim \mathcal{N}(0, \sigma_{\alpha G}^2) \\
\alpha_T &\sim \mathcal{N}(0, 0.05) \\
\alpha_{TG} &\sim \mathcal{N}(0, \sigma_{\alpha TG}^2) \\
\delta &\sim \alpha + \alpha_{G} + (\alpha_{T} + \alpha_{TG}) \times trt
\end{align*}
$$

The parameters and model of the Stan program is

```
parameters {
  real alpha;
  real alpha_G[N_G];
  real alpha_T;
  real alpha_TG[N_G];
  
  real beta;
  real beta_G[N_G];
  real beta_T;
  real beta_TG[N_G];
  
  real<lower=machine_precision()> sigma_aG;
  real<lower=machine_precision()> sigma_aTG;
  real<lower=machine_precision()> sigma_bG;
  real<lower=machine_precision()> sigma_bTG;
}

model {
  alpha ~ normal(0, 0.05);
  alpha_G ~ normal(0, sigma_aG);
  alpha_T ~ normal(0, 0.05);
  alpha_TG ~ normal(0, sigma_aTG);
  
  beta ~ normal(3.96, 1.2);
  beta_G ~ normal(0, sigma_bG);
  beta_T ~ normal(0, 1);
  beta_TG ~ normal(0, sigma_bTG);
  
  sigma_aG ~ cauchy(0, 0.1);
  sigma_aTG ~ cauchy(0, 0.1);
  sigma_bG ~ cauchy(0, 1.0);
  sigma_bTG ~ cauchy(0, 1.0);
  
  vector[N] p;
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    p[i] = gamma * (x[i] - delta);
  }
  k ~ binomial_logit(n, p); // Observational model
}
```




###  Simulate bayesian ensemble

```{stan output.var="sim_iter2"}
functions {
  real half_cauchy_rng(real sigma) {
    real u = uniform_rng(0.5, 0.99);
    real y = sigma * tan(pi() * (u - 0.5));
    return y;
  }
}
data {
  int N;
  int N_G;
  int n[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
generated quantities {
  vector[N] theta;
  int y_sim[N];
  vector[N_G] pss_pre;
  vector[N_G] pss_post;
  vector[N_G] jnd_pre;
  vector[N_G] jnd_post;
  
  real alpha = normal_rng(0, 0.05);
  real alpha_T = normal_rng(0, 0.025);
  real<lower=0> sigma_aG = half_cauchy_rng(0.005);
  real<lower=0> sigma_aTG = half_cauchy_rng(0.005);
  real alpha_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_aG);
  real alpha_TG[N_G] = normal_rng(rep_vector(0, N_G), sigma_aTG);
  
  real beta = normal_rng(3.96, 1.0);
  real beta_T = normal_rng(0, 0.1);
  real<lower=0> sigma_bG = half_cauchy_rng(0.05);
  real<lower=0> sigma_bTG = half_cauchy_rng(0.05);
  real beta_G[N_G] = normal_rng(rep_vector(0, N_G), sigma_bG);
  real beta_TG[N_G] = normal_rng(rep_vector(0, N_G), sigma_bTG);
  
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    
    theta[i] = inv_logit( gamma * (x[i] - delta) );
    y_sim[i] = binomial_rng(n[i], theta[i]);
  }
  
  for (i in 1:N_G) {
    real gamma_pre = exp(beta + beta_G[i]);
    real gamma_post = exp(beta + beta_G[i] + beta_T + beta_TG[i]);
    real delta_pre = alpha + alpha_G[i];
    real delta_post = alpha + alpha_G[i] + alpha_T + alpha_TG[i];
    
    pss_pre[i] = delta_pre;
    pss_post[i] = delta_post;
    jnd_pre[i] = logit(0.84) / gamma_pre;
    jnd_post[i] = logit(0.84) / gamma_post;
  }
}
```


```{r}
av_dat <- audiovisual_binomial %>%
  filter(trial %in% c("pre", "post1"),
         rid != "av-post1-O-f-CE") %>%
  mutate(trt = as.integer(trial != "pre"))

sim_dat2 = list(N = nrow(av_dat),
                N_G = max(as.integer(av_dat$age_group)),
                n = av_dat$n,
                x = av_dat$soa / 1000,
                G = as.integer(av_dat$age_group),
                trt = av_dat$trt)
```


```{r}
sim_fit2 <- sampling(sim_iter2, data = sim_dat2, 
                     algorithm="Fixed_param", refresh = 2000)
sim_prior2 <- extract(sim_fit2)
```

### Prior Checks

```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(sim_prior2$pss_pre[,i], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(sim_prior2$pss_post[,i], breaks = 50, probability = TRUE)
}
```


```{r}
par(mfrow=c(2,3))
for (i in 1:3) {
  hist(sim_prior2$jnd_pre[,i], breaks = 50, probability = TRUE)
}
for (i in 1:3) {
  hist(sim_prior2$jnd_post[,i], breaks = 50, probability = TRUE)
}
```

```{r}
apply(sim_prior2$jnd_pre, 2, quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>%  round(3)
apply(sim_prior2$jnd_post, 2, quantile, probs = c(0.5, 0.95, 0.99, 0.999)) %>%  round(3)
```

### Configure algorithm

### Fit simulated ensemble

```{stan output.var="fit_sim_iter2"}
data {
  int N;
  int N_G;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real alpha;
  real alpha_T;
  real<lower=0> sigma_aG;
  real<lower=0> sigma_aTG;
  real alpha_G[N_G];
  real alpha_TG[N_G];
  
  real beta;
  real beta_T;
  real<lower=0> sigma_bG;
  real<lower=0> sigma_bTG;
  real beta_G[N_G];
  real beta_TG[N_G];
}
model {
  vector[N] theta;

  alpha     ~ normal(0, 0.05);
  alpha_G   ~ normal(0, sigma_aG);
  
  alpha_T   ~ normal(0, 0.025);
  alpha_TG  ~ normal(0, sigma_aTG);
  
  sigma_aG  ~ cauchy(0, 0.005);
  sigma_aTG ~ cauchy(0, 0.005);
  
  beta      ~ normal(3.96, 1.0);
  beta_G    ~ normal(0, sigma_bG);
  
  beta_T    ~ normal(0, 1);
  beta_TG   ~ normal(0, sigma_bTG);
  
  sigma_bG  ~ cauchy(0, 2.5);
  sigma_bTG ~ cauchy(0, 2.5);
  
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    theta[i] = gamma * (x[i] - delta);
  }
  
  k ~ binomial_logit(n, theta);
}
generated quantities {
  vector[N_G] pss_pre;
  vector[N_G] pss_post;
  vector[N_G] jnd_pre;
  vector[N_G] jnd_post;
  
  for (i in 1:N_G) {
    real gamma_pre = exp(beta + beta_G[i]);
    real gamma_post = exp(beta + beta_G[i] + beta_T + beta_TG[i]);
    real delta_pre = alpha + alpha_G[i];
    real delta_post = alpha + alpha_G[i] + alpha_T + alpha_TG[i];
    
    pss_pre[i] = delta_pre;
    pss_post[i] = delta_post;
    jnd_pre[i] = logit(0.84) / gamma_pre;
    jnd_post[i] = logit(0.84) / gamma_post;
  }
}
```


```{r}
sim_dat2$k <- sapply(1:ncol(sim_prior2$y_sim), function(i) {
  sim_prior2$y_sim[sample(4000, 1), i]
})
```


Since we are dealing with the log-normal distribution, we need to be careful and specifiy initial values for sampling.


```{r}
sim_fit2 <- sampling(fit_sim_iter2, data = sim_dat2, refresh = 1000)
sim_post2 <- extract(sim_fit2)
```


### Algorithmic calibration

```{r}
check_hmc_diagnostics(sim_fit2)
```

Additionally we were given the warning that the Bulk ESS is too low, and that running the chains for more iterations can help. So we do just that, and also increase the adapt delta.

```{r}
sim_fit2 <- sampling(fit_sim_iter2, data = sim_dat2, iter = 4000, warmup = 1000,
                     refresh = 1000, control = list(adapt_delta = 0.95))
sim_post2 <- extract(sim_fit2)
```

```{r}
summary(sim_fit2)$summary %>% round(4)
```

### Inferential Calibration

### Fit Observation

```{stan output.var="fit_obs_iter2"}
data {
  int N;
  int N_G;
  int n[N];
  int k[N];
  vector[N] x;
  int G[N];
  int trt[N];
}
parameters {
  real alpha;
  real alpha_T;
  real<lower=0> sigma_aG;
  real<lower=0> sigma_aTG;
  real alpha_G[N_G];
  real alpha_TG[N_G];
  
  real beta;
  real beta_T;
  real<lower=0> sigma_bG;
  real<lower=0> sigma_bTG;
  real beta_G[N_G];
  real beta_TG[N_G];
}
model {
  vector[N] theta;

  alpha     ~ normal(0, 1);
  alpha_G   ~ normal(0, sigma_aG);
  
  alpha_T   ~ normal(0, 1);
  alpha_TG  ~ normal(0, sigma_aTG);
  
  sigma_aG  ~ cauchy(0, 2.5);
  sigma_aTG ~ cauchy(0, 2.5);
  
  beta      ~ normal(5, 10);
  beta_G    ~ normal(0, sigma_bG);
  
  beta_T    ~ normal(0, 10);
  beta_TG   ~ normal(0, sigma_bTG);
  
  sigma_bG  ~ cauchy(0, 5);
  sigma_bTG ~ cauchy(0, 5);
  
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    theta[i] = gamma * (x[i] - delta);
  }
  
  k ~ binomial_logit(n, theta);
}
generated quantities {
  int y_post_pred[N];
  vector[N_G] pss_pre;
  vector[N_G] pss_post;
  vector[N_G] jnd_pre;
  vector[N_G] jnd_post;
  
  for (i in 1:N_G) {
    real gamma_pre = exp(beta + beta_G[i]);
    real gamma_post = exp(beta + beta_G[i] + beta_T + beta_TG[i]);
    real delta_pre = alpha + alpha_G[i];
    real delta_post = alpha + alpha_G[i] + alpha_T + alpha_TG[i];
    
    pss_pre[i] = delta_pre;
    pss_post[i] = delta_post;
    jnd_pre[i] = logit(0.84) / gamma_pre;
    jnd_post[i] = logit(0.84) / gamma_post;
  }
  
  for (i in 1:N) {
    real gamma = exp(beta + beta_G[G[i]] + (beta_T + beta_TG[G[i]]) * trt[i]);
    real delta = alpha + alpha_G[G[i]] + (alpha_T + alpha_TG[G[i]]) * trt[i];
    y_post_pred[i] = binomial_rng(n[i], inv_logit(beta * (x[i] - alpha)));
  }
}
```




```{r}
obs_dat2 <- list(N = nrow(av_dat),
                 N_G = max(as.integer(av_dat$age_group)),
                 x = av_dat$soa / 1000,
                 k = av_dat$k,
                 n = av_dat$n,
                 G = as.integer(av_dat$age_group),
                 trt = av_dat$trt)

m2_fit <- sampling(fit_obs_iter2, data = obs_dat2, iter = 5000, warmup = 2500,
                   refresh = 2500, control = list(adapt_delta=0.95, max_treedepth=12))
```


### Diagnose posterior fit

### Posterior retrodictive checks

```{r}
m2_post <- extract(m2_fit)
m2_summ <- summary(m2_fit)$summary
```

```{r}
m2_post_k_pred <- t(apply(m2_post$y_post_pred, 2, quantile,
                          probs = c(1.5, 5.5, 50, 94.5, 98.5) / 100))

m2_pred <- cbind(m2_post_k_pred, post_mean = colMeans(m2_post$y_post_pred)) %>%
  sweep(1, obs_dat2$n, FUN = "/") %>%
  bind_cols(obs_dat2) %>%
  select(-N) %>%
  mutate(p = k / n)

print(m2_pred, n = 21)
```


```{r}
m2_pred %>%
  ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = `1.5%`, ymax = `98.5%`), alpha = 0.05) +
  geom_ribbon(aes(ymin = `5.5%`, ymax = `94.5%`), alpha = 0.15) +
  geom_point(alpha = 0.1) +
  geom_point(aes(y = `50%`), col = "green", alpha = 0.5) +
  facet_grid(G~trt)
```
